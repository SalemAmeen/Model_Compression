{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.model_pruning.python import pruning\n",
    "from tensorflow.contrib.model_pruning.python.layers import layers\n",
    "import keras\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta1_power (DT_FLOAT) []\n",
      "beta2_power (DT_FLOAT) []\n",
      "fcn/conv1-1/bias (DT_FLOAT) [96]\n",
      "fcn/conv1-1/bias/Adam (DT_FLOAT) [96]\n",
      "fcn/conv1-1/bias/Adam_1 (DT_FLOAT) [96]\n",
      "fcn/conv1-1/kernel (DT_FLOAT) [3,3,3,96]\n",
      "fcn/conv1-1/kernel/Adam (DT_FLOAT) [3,3,3,96]\n",
      "fcn/conv1-1/kernel/Adam_1 (DT_FLOAT) [3,3,3,96]\n",
      "fcn/conv1-2/bias (DT_FLOAT) [96]\n",
      "fcn/conv1-2/bias/Adam (DT_FLOAT) [96]\n",
      "fcn/conv1-2/bias/Adam_1 (DT_FLOAT) [96]\n",
      "fcn/conv1-2/kernel (DT_FLOAT) [3,3,96,96]\n",
      "fcn/conv1-2/kernel/Adam (DT_FLOAT) [3,3,96,96]\n",
      "fcn/conv1-2/kernel/Adam_1 (DT_FLOAT) [3,3,96,96]\n",
      "fcn/conv2-1/bias (DT_FLOAT) [192]\n",
      "fcn/conv2-1/bias/Adam (DT_FLOAT) [192]\n",
      "fcn/conv2-1/bias/Adam_1 (DT_FLOAT) [192]\n",
      "fcn/conv2-1/kernel (DT_FLOAT) [3,3,96,192]\n",
      "fcn/conv2-1/kernel/Adam (DT_FLOAT) [3,3,96,192]\n",
      "fcn/conv2-1/kernel/Adam_1 (DT_FLOAT) [3,3,96,192]\n",
      "fcn/conv2-2/bias (DT_FLOAT) [192]\n",
      "fcn/conv2-2/bias/Adam (DT_FLOAT) [192]\n",
      "fcn/conv2-2/bias/Adam_1 (DT_FLOAT) [192]\n",
      "fcn/conv2-2/kernel (DT_FLOAT) [3,3,192,192]\n",
      "fcn/conv2-2/kernel/Adam (DT_FLOAT) [3,3,192,192]\n",
      "fcn/conv2-2/kernel/Adam_1 (DT_FLOAT) [3,3,192,192]\n",
      "fcn/conv3/bias (DT_FLOAT) [192]\n",
      "fcn/conv3/bias/Adam (DT_FLOAT) [192]\n",
      "fcn/conv3/bias/Adam_1 (DT_FLOAT) [192]\n",
      "fcn/conv3/kernel (DT_FLOAT) [3,3,192,192]\n",
      "fcn/conv3/kernel/Adam (DT_FLOAT) [3,3,192,192]\n",
      "fcn/conv3/kernel/Adam_1 (DT_FLOAT) [3,3,192,192]\n",
      "fcn/conv4/bias (DT_FLOAT) [192]\n",
      "fcn/conv4/bias/Adam (DT_FLOAT) [192]\n",
      "fcn/conv4/bias/Adam_1 (DT_FLOAT) [192]\n",
      "fcn/conv4/kernel (DT_FLOAT) [1,1,192,192]\n",
      "fcn/conv4/kernel/Adam (DT_FLOAT) [1,1,192,192]\n",
      "fcn/conv4/kernel/Adam_1 (DT_FLOAT) [1,1,192,192]\n",
      "fcn/conv5/bias (DT_FLOAT) [10]\n",
      "fcn/conv5/bias/Adam (DT_FLOAT) [10]\n",
      "fcn/conv5/bias/Adam_1 (DT_FLOAT) [10]\n",
      "fcn/conv5/kernel (DT_FLOAT) [1,1,192,10]\n",
      "fcn/conv5/kernel/Adam (DT_FLOAT) [1,1,192,10]\n",
      "fcn/conv5/kernel/Adam_1 (DT_FLOAT) [1,1,192,10]\n",
      "fcn/norm1-1/beta (DT_FLOAT) [96]\n",
      "fcn/norm1-1/beta/Adam (DT_FLOAT) [96]\n",
      "fcn/norm1-1/beta/Adam_1 (DT_FLOAT) [96]\n",
      "fcn/norm1-1/gamma (DT_FLOAT) [96]\n",
      "fcn/norm1-1/gamma/Adam (DT_FLOAT) [96]\n",
      "fcn/norm1-1/gamma/Adam_1 (DT_FLOAT) [96]\n",
      "fcn/norm1-1/moving_mean (DT_FLOAT) [96]\n",
      "fcn/norm1-1/moving_variance (DT_FLOAT) [96]\n",
      "fcn/norm1-2/beta (DT_FLOAT) [96]\n",
      "fcn/norm1-2/beta/Adam (DT_FLOAT) [96]\n",
      "fcn/norm1-2/beta/Adam_1 (DT_FLOAT) [96]\n",
      "fcn/norm1-2/gamma (DT_FLOAT) [96]\n",
      "fcn/norm1-2/gamma/Adam (DT_FLOAT) [96]\n",
      "fcn/norm1-2/gamma/Adam_1 (DT_FLOAT) [96]\n",
      "fcn/norm1-2/moving_mean (DT_FLOAT) [96]\n",
      "fcn/norm1-2/moving_variance (DT_FLOAT) [96]\n",
      "fcn/norm2-1/beta (DT_FLOAT) [192]\n",
      "fcn/norm2-1/beta/Adam (DT_FLOAT) [192]\n",
      "fcn/norm2-1/beta/Adam_1 (DT_FLOAT) [192]\n",
      "fcn/norm2-1/gamma (DT_FLOAT) [192]\n",
      "fcn/norm2-1/gamma/Adam (DT_FLOAT) [192]\n",
      "fcn/norm2-1/gamma/Adam_1 (DT_FLOAT) [192]\n",
      "fcn/norm2-1/moving_mean (DT_FLOAT) [192]\n",
      "fcn/norm2-1/moving_variance (DT_FLOAT) [192]\n",
      "fcn/norm2-2/beta (DT_FLOAT) [192]\n",
      "fcn/norm2-2/beta/Adam (DT_FLOAT) [192]\n",
      "fcn/norm2-2/beta/Adam_1 (DT_FLOAT) [192]\n",
      "fcn/norm2-2/gamma (DT_FLOAT) [192]\n",
      "fcn/norm2-2/gamma/Adam (DT_FLOAT) [192]\n",
      "fcn/norm2-2/gamma/Adam_1 (DT_FLOAT) [192]\n",
      "fcn/norm2-2/moving_mean (DT_FLOAT) [192]\n",
      "fcn/norm2-2/moving_variance (DT_FLOAT) [192]\n",
      "fcn/norm3/beta (DT_FLOAT) [192]\n",
      "fcn/norm3/beta/Adam (DT_FLOAT) [192]\n",
      "fcn/norm3/beta/Adam_1 (DT_FLOAT) [192]\n",
      "fcn/norm3/gamma (DT_FLOAT) [192]\n",
      "fcn/norm3/gamma/Adam (DT_FLOAT) [192]\n",
      "fcn/norm3/gamma/Adam_1 (DT_FLOAT) [192]\n",
      "fcn/norm3/moving_mean (DT_FLOAT) [192]\n",
      "fcn/norm3/moving_variance (DT_FLOAT) [192]\n",
      "fcn/norm4/beta (DT_FLOAT) [192]\n",
      "fcn/norm4/beta/Adam (DT_FLOAT) [192]\n",
      "fcn/norm4/beta/Adam_1 (DT_FLOAT) [192]\n",
      "fcn/norm4/gamma (DT_FLOAT) [192]\n",
      "fcn/norm4/gamma/Adam (DT_FLOAT) [192]\n",
      "fcn/norm4/gamma/Adam_1 (DT_FLOAT) [192]\n",
      "fcn/norm4/moving_mean (DT_FLOAT) [192]\n",
      "fcn/norm4/moving_variance (DT_FLOAT) [192]\n",
      "fcn/norm5/beta (DT_FLOAT) [10]\n",
      "fcn/norm5/beta/Adam (DT_FLOAT) [10]\n",
      "fcn/norm5/beta/Adam_1 (DT_FLOAT) [10]\n",
      "fcn/norm5/gamma (DT_FLOAT) [10]\n",
      "fcn/norm5/gamma/Adam (DT_FLOAT) [10]\n",
      "fcn/norm5/gamma/Adam_1 (DT_FLOAT) [10]\n",
      "fcn/norm5/moving_mean (DT_FLOAT) [10]\n",
      "fcn/norm5/moving_variance (DT_FLOAT) [10]\n",
      "global_step (DT_INT64) []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "print_tensors_in_checkpoint_file('Models_Given/model-50000', all_tensors=False, all_tensor_names=False, tensor_name='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 250 # Entire training set\n",
    "model_path_unpruned = \"Model_Saves/Unpruned.ckpt\"\n",
    "model_path_pruned = \"Model_Saves/Pruned.ckpt\"\n",
    "NUM_CLASS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading numpy\n",
      "Loading numpy\n"
     ]
    }
   ],
   "source": [
    "#Load Dataset\n",
    "X_train, y_train = prepare_dataset(data_dir, 'train')\n",
    "X_test, y_test = prepare_dataset(data_dir, 'test')\n",
    "t = int(time.time())\n",
    "#Labels to binary\n",
    "y_train_binary = keras.utils.to_categorical(y_train,num_classes)\n",
    "y_test_binary = keras.utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "batches = int(len(X_train) / batch_size)\n",
    "print(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training datasets\n",
    "dx_train = tf.data.Dataset.from_tensor_slices(X_train.astype(np.float32))\n",
    "# apply a one-hot transformation to each label for use in the neural network\n",
    "dy_train = tf.data.Dataset.from_tensor_slices(y_train).map(lambda z: tf.one_hot(z, NUM_CLASS))\n",
    "# zip the x and y training data together and shuffle, batch etc.\n",
    "train_dataset = tf.data.Dataset.zip((dx_train, dy_train)).shuffle(500).repeat().batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same operations for the validation set\n",
    "dx_test = tf.data.Dataset.from_tensor_slices(X_test.astype(np.float32))\n",
    "dy_test = tf.data.Dataset.from_tensor_slices(y_test).map(lambda z: tf.one_hot(z, NUM_CLASS))\n",
    "test_dataset = tf.data.Dataset.zip((dx_test, dy_test)).shuffle(500).repeat().batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "test_iterator = test_dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create general iterator\n",
    "# iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "#                                                train_dataset.output_shapes)\n",
    "# next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make datasets that we can initialize separately, but using the same structure via the common iterator\n",
    "# training_init_op = iterator.make_initializer(train_dataset)\n",
    "# validation_init_op = iterator.make_initializer(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nn_model(in_data):\n",
    "#     bn = tf.layers.batch_normalization(in_data)\n",
    "#     fc1 = tf.layers.dense(bn, 50)\n",
    "#     fc2 = tf.layers.dense(fc1, 50)\n",
    "#     fc2 = tf.layers.dropout(fc2)\n",
    "#     fc3 = tf.layers.dense(fc2, 10)\n",
    "#     return fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create the neural network model\n",
    "# logits = nn_model(next_element[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add the optimizer and loss\n",
    "# loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=next_element[1], logits=logits))\n",
    "# optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "# # get accuracy\n",
    "# prediction = tf.argmax(logits, 1)\n",
    "# equality = tf.equal(prediction, tf.argmax(next_element[1], 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "# init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_input_fn(features, labels, batch_size):\n",
    "#     \"\"\"An input function for training\"\"\"\n",
    "#     # Convert the inputs to a Dataset.\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "#     # Shuffle, repeat, and batch the examples.\n",
    "#     dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "#     # Return the dataset.\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run the training\n",
    "# epochs = 10\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init_op)\n",
    "#     sess.run(training_init_op)\n",
    "#     for i in range(epochs):\n",
    "#         l, _, acc = sess.run([loss, optimizer, accuracy])\n",
    "#         if i % 50 == 0:\n",
    "#             print(\"Epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%\".format(i, l, acc * 100))\n",
    "#     # now setup the validation run\n",
    "#     valid_iters = 100\n",
    "#     # re-initialize the iterator, but this time with validation data\n",
    "#     sess.run(validation_init_op)\n",
    "#     avg_acc = 0\n",
    "#     for i in range(valid_iters):\n",
    "#         acc = sess.run([accuracy])\n",
    "#         avg_acc += acc[0]\n",
    "#     print(\"Average validation set accuracy over {} iterations is {:.2f}%\".format(valid_iters, (avg_acc / valid_iters) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() got an unexpected keyword argument 'activation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-347b9a242f71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#_ = layers.masked_conv2d(_, 96, (3, 3), 1, 'SAME')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m _ = tf.nn.conv2d(image, pruning.apply_mask(tf.Variable(tf.random_normal([3,3,3,96]))), [1,1,1,1], 'SAME', \n\u001b[1;32m---> 19\u001b[1;33m                   activation=tf.nn.leaky_relu, name = 'conv1-1')\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_normalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'norm1-1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#_ = layers.masked_conv2d(_, 96, (3, 3), 1, 'SAME')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() got an unexpected keyword argument 'activation'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "image = tf.placeholder(name='images', dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "label = tf.placeholder(name='fine_labels', dtype=tf.int32, shape=[None, 10])\n",
    "\n",
    "# # Define the model\n",
    "# layer1 = layers.masked_fully_connected(image, 300)\n",
    "# layer2 = layers.masked_fully_connected(layer1, 300)\n",
    "# logits = layers.masked_fully_connected(layer2, 10)\n",
    "\n",
    "# Define the model\n",
    "# layer1 = layers.masked_conv2d(image, 300, kernel_size=2)\n",
    "# layer2 = layers.masked_conv2d(layer1, 300, kernel_size=2)\n",
    "# logits = layers.masked_fully_connected(layer2, 10)\n",
    "\n",
    "_=image\n",
    "#_ = layers.masked_conv2d(_, 96, (3, 3), 1, 'SAME')\n",
    "_ = tf.nn.conv2d(image, pruning.apply_mask(tf.Variable(tf.random_normal([3,3,3,96]))), [1,1,1,1], 'SAME', \n",
    "                  activation=tf.nn.leaky_relu, name = 'conv1-1')\n",
    "_ = tf.layers.batch_normalization(_, name='norm1-1')\n",
    "#_ = layers.masked_conv2d(_, 96, (3, 3), 1, 'SAME')\n",
    "_ = tf.nn.conv2d(image, pruning.apply_mask(tf.Variable(tf.random_normal([3,3,96,96]))), [1,1,1,1], 'SAME',\n",
    "                  activation=tf.nn.leaky_relu, name = 'conv1-2')\n",
    "_ = tf.layers.batch_normalization(_, name='norm1-2')\n",
    "_ = tf.layers.max_pooling2d(_, (3, 3), 2, 'SAME',name='pool1')\n",
    "#_ = layers.masked_conv2d(_, 192, (3, 3), 1, 'SAME')\n",
    "_ = tf.nn.conv2d(image, pruning.apply_mask(tf.Variable(tf.random_normal([3,3,96,192]))), [1,1,1,1], 'SAME',\n",
    "                  activation=tf.nn.leaky_relu, name = 'conv2-1')\n",
    "_ = tf.layers.batch_normalization(_, name='norm2-1')\n",
    "#_ = layers.masked_conv2d(_, 192, (3, 3), 1, 'SAME')\n",
    "_ = tf.nn.conv2d(image, pruning.apply_mask(tf.Variable(tf.random_normal([3,3,192,192]))), [1,1,1,1], 'SAME',\n",
    "                 activation=tf.nn.leaky_relu, name = 'conv2-2')\n",
    "_ = tf.layers.batch_normalization(_, name='norm2-2')\n",
    "_ = tf.layers.max_pooling2d(_, (3, 3), 2, 'SAME', name='pool2')\n",
    "#_ = layers.masked_conv2d(_, 192, (3, 3), 1, 'VALID')\n",
    "_ = tf.nn.conv2d(image, pruning.apply_mask(tf.Variable(tf.random_normal([3,3,192,192]))), [1,1,1,1], 'VALID',\n",
    "                 activation=tf.nn.leaky_relu, name = 'conv3')\n",
    "_ = tf.layers.batch_normalization(_, name='norm3')\n",
    "#_ = layers.masked_conv2d(_, 192, (1, 1), 1)\n",
    "_ = tf.nn.conv2d(image, pruning.apply_mask(tf.Variable(tf.random_normal([1,1,192,192]))), [1,1,1,1], 'SAME',\n",
    "                 activation=tf.nn.leaky_relu, name = 'conv4')\n",
    "_ = tf.layers.batch_normalization(_, name='norm4')\n",
    "_ = layers.masked_conv2d(_, 10, (1, 1), 1)\n",
    "_ = tf.nn.conv2d(image, pruning.apply_mask(tf.Variable(tf.random_normal([3,3,3,96]))), [1,1,1,1], 'SAME',\n",
    "                 activation=tf.nn.leaky_relu, name = 'conv5')\n",
    "_ = tf.layers.batch_normalization(_, name='norm5')\n",
    "_ = tf.layers.average_pooling2d(_, (6,6), 1, name='avg_pool')\n",
    "y = _\n",
    "logits = tf.reshape(y,[tf.shape(y)[0],10])\n",
    "\n",
    "\n",
    "# Create global step variable (needed for pruning)\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "reset_global_step_op = tf.assign(global_step, 0)\n",
    "\n",
    "# Loss function\n",
    "#loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=label))\n",
    "#loss = tf.losses.softmax_cross_entropy(tf.one_hot(label, NUM_CLASS), logits)\n",
    "loss = tf.losses.softmax_cross_entropy(label, logits)\n",
    "\n",
    "# Training op, the global step is critical here, make sure it matches the one used in pruning later\n",
    "# running this operation increments the global_step\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss, global_step=global_step)\n",
    "\n",
    "# Accuracy ops\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning Hyperparameters: [('begin_pruning_step', 0), ('block_height', 1), ('block_pooling_function', 'AVG'), ('block_width', 1), ('do_not_prune', ['']), ('end_pruning_step', -1), ('initial_sparsity', 0), ('name', 'model_pruning'), ('nbins', 256), ('pruning_frequency', 10), ('sparsity_function_begin_step', 0), ('sparsity_function_end_step', 100), ('sparsity_function_exponent', 3), ('target_sparsity', 0.5), ('threshold_decay', 0.9), ('use_tpu', False)]\n",
      "INFO:tensorflow:Updating masks.\n"
     ]
    }
   ],
   "source": [
    "# Get, Print, and Edit Pruning Hyperparameters\n",
    "pruning_hparams = pruning.get_pruning_hparams()\n",
    "print(\"Pruning Hyperparameters:\", pruning_hparams)\n",
    "\n",
    "# Change hyperparameters to meet our needs\n",
    "pruning_hparams.begin_pruning_step = 0\n",
    "pruning_hparams.end_pruning_step = 250\n",
    "pruning_hparams.pruning_frequency = 1\n",
    "pruning_hparams.sparsity_function_end_step = 250\n",
    "pruning_hparams.target_sparsity = .5\n",
    "\n",
    "# Create a pruning object using the pruning specification, sparsity seems to have priority over the hparam\n",
    "p = pruning.Pruning(pruning_hparams, global_step=global_step) #sparsity=.5)\n",
    "prune_op = p.conditional_mask_update_op()\n",
    "\n",
    "# Create a saver for writing training checkpoints.\n",
    "saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "Un-pruned model step 0 test accuracy 0.137079\n",
      "0\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "Un-pruned model step 1 test accuracy 0.220072\n",
      "1\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "(256, 32, 32, 3)\n",
      "Un-pruned model step 2 test accuracy 0.316186\n",
      "2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-70341807203c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path_unpruned\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0macc_print\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pre-Pruning accuracy:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_print\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sparsity of layers (should be 0)\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_pruning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weight_sparsity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    # Uncomment the following if you don't have a trained model yet\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Train the model before pruning (optional)\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(batches):\n",
    "            #batch_xs, batch_ys = train_iterator.get_next()\n",
    "            batch_xs, batch_ys = sample_batch(X_train, y_train_binary, batch_size)\n",
    "            #print(batch_xs.shape)\n",
    "            #batch_xs = batch_xs.reshape(-1,28,28,1)\n",
    "            sess.run(train_op, feed_dict={image: batch_xs, label: batch_ys})\n",
    "\n",
    "        # Calculate Test Accuracy every 10 epochs\n",
    "        if epoch % 1 == 0:\n",
    "            acc_print = 0\n",
    "            for batch in range(batches):\n",
    "                #batch_xt, batch_yt = test_iterator.get_next()\n",
    "                batch_xt, batch_yt = sample_batch(X_test, y_test_binary, batch_size)\n",
    "                print(batch_xt.shape)\n",
    "                #batch_xt = batch_xt.reshape(-1,28,28,1)\n",
    "                acc_print += sess.run(accuracy, feed_dict={image: batch_xt, label: batch_yt})\n",
    "            print(\"Un-pruned model step %d test accuracy %g\" % (epoch, acc_print/batches))\n",
    "        print(epoch)\n",
    "    \n",
    "    # Saves the model before pruning\n",
    "    saver.save(sess, model_path_unpruned)\n",
    "    \n",
    "    acc_print = sess.run(accuracy, feed_dict={image: mnist.test.images.reshape(-1,28,28,1), label: mnist.test.labels})\n",
    "    print(\"Pre-Pruning accuracy:\", acc_print)\n",
    "    print(\"Sparsity of layers (should be 0)\", sess.run(tf.contrib.model_pruning.get_weight_sparsity()))\n",
    "\n",
    "    \n",
    "\n",
    "    # Resets the session and restores the saved model\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    saver.restore(sess, model_path_unpruned)\n",
    "\n",
    "    # Reset the global step counter and begin pruning\n",
    "    sess.run(reset_global_step_op)\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            batch_xs = batch_xs.reshape(-1,28,28,1)\n",
    "            # Prune and retrain\n",
    "            sess.run(prune_op)\n",
    "            sess.run(train_op, feed_dict={image: batch_xs, label: batch_ys})\n",
    "\n",
    "        # Calculate Test Accuracy every 10 epochs\n",
    "        if epoch % 1 == 0:\n",
    "            acc_print = 0\n",
    "            for batch in range(batches):\n",
    "                batch_xt, batch_yt = mnist.test.next_batch(batch_size)\n",
    "                #print(batch_xt.shape)\n",
    "                batch_xt = batch_xt.reshape(-1,28,28,1)\n",
    "                acc_print += sess.run(accuracy, feed_dict={image: batch_xt, label: batch_yt})\n",
    "            \n",
    "            print(\"Pruned model step %d test accuracy %g\" % (epoch, acc_print/batches))\n",
    "            print(\"Weight sparsities:\", sess.run(tf.contrib.model_pruning.get_weight_sparsity()))\n",
    "        print(epoch) \n",
    "        \n",
    "           # acc_print = sess.run(accuracy, feed_dict={image: mnist.test.images, label: mnist.test.labels})\n",
    "\n",
    "    # Saves the model after pruning\n",
    "    saver.save(sess, model_path_pruned)\n",
    "\n",
    "    # Print final accuracy\n",
    "    #acc_print = sess.run(accuracy, feed_dict={image: mnist.test.images, label: mnist.test.labels})\n",
    "    print(\"Final accuracy:\", acc_print)\n",
    "    print(\"Final sparsity by layer (should be 0)\", sess.run(tf.contrib.model_pruning.get_weight_sparsity()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(\"Const_1:0\", shape=(), dtype=string)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-9559a232e0c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'conv1-1'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Conv/weights:0'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[0;32m   1282\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[0;32m   1283\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1284\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1285\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m_build\u001b[1;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[0;32m   1331\u001b[0m           \u001b[0mrestore_sequentially\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restore_sequentially\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m           \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1333\u001b[1;33m           build_save=build_save, build_restore=build_restore)\n\u001b[0m\u001b[0;32m   1334\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m       \u001b[1;31m# Since self._name is used as a name_scope by builder(), we are\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m_build_internal\u001b[1;34m(self, names_to_saveables, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, filename, build_save, build_restore)\u001b[0m\n\u001b[0;32m    757\u001b[0m                        \" when eager execution is not enabled.\")\n\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m     \u001b[0msaveables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ValidateAndSliceInputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmax_to_keep\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m       \u001b[0mmax_to_keep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m_ValidateAndSliceInputs\u001b[1;34m(self, names_to_saveables)\u001b[0m\n\u001b[0;32m    664\u001b[0m                            \u001b[1;31m# Avoid comparing ops, sort only by name.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m                            key=lambda x: x[0]):\n\u001b[1;32m--> 666\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0mconverted_saveable_object\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaveableObjectsForOp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AddSaveable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaveables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseen_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverted_saveable_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msaveables\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mSaveableObjectsForOp\u001b[1;34m(op, name)\u001b[0m\n\u001b[0;32m    632\u001b[0m           raise TypeError(\"names_to_saveables must be a dict mapping string \"\n\u001b[0;32m    633\u001b[0m                           \u001b[1;34m\"names to Tensors/Variables. Not a variable: %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m                           variable)\n\u001b[0m\u001b[0;32m    635\u001b[0m         if variable.op.type in [\"Variable\", \"VariableV2\",\n\u001b[0;32m    636\u001b[0m                                 \"AutoReloadVariable\"]:\n",
      "\u001b[1;31mTypeError\u001b[0m: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(\"Const_1:0\", shape=(), dtype=string)"
     ]
    }
   ],
   "source": [
    "# tf.reset_default_graph()\n",
    "# saver = tf.train.Saver(var_list={'v1': b1})\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "image = tf.placeholder(name='images', dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "label = tf.placeholder(name='fine_labels', dtype=tf.int32, shape=[None, 10])\n",
    "\n",
    "_=image\n",
    "_ = layers.masked_conv2d(_, 96, (3, 3), 1, 'SAME')\n",
    "_ = tf.layers.batch_normalization(_, name='norm1-1')\n",
    "_ = layers.masked_conv2d(_, 96, (3, 3), 1, 'SAME')\n",
    "_ = tf.layers.batch_normalization(_, name='norm1-2')\n",
    "_ = tf.layers.max_pooling2d(_, (3, 3), 2, 'SAME',name='pool1')\n",
    "_ = layers.masked_conv2d(_, 192, (3, 3), 1, 'SAME')\n",
    "_ = tf.layers.batch_normalization(_, name='norm2-1')\n",
    "_ = layers.masked_conv2d(_, 192, (3, 3), 1, 'SAME')\n",
    "_ = tf.layers.batch_normalization(_, name='norm2-2')\n",
    "_ = tf.layers.max_pooling2d(_, (3, 3), 2, 'SAME', name='pool2')\n",
    "_ = layers.masked_conv2d(_, 192, (3, 3), 1, 'VALID')\n",
    "_ = tf.layers.batch_normalization(_, name='norm3')\n",
    "_ = layers.masked_conv2d(_, 192, (1, 1), 1)\n",
    "_ = tf.layers.batch_normalization(_, name='norm4')\n",
    "_ = layers.masked_conv2d(_, 10, (1, 1), 1)\n",
    "_ = tf.layers.batch_normalization(_, name='norm5')\n",
    "_ = tf.layers.average_pooling2d(_, (6,6), 1, name='avg_pool')\n",
    "y = _\n",
    "logits = tf.reshape(y,[tf.shape(y)[0],10])\n",
    "\n",
    "\n",
    "# Create global step variable (needed for pruning)\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "reset_global_step_op = tf.assign(global_step, 0)\n",
    "\n",
    "# Loss function\n",
    "#loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=label))\n",
    "#loss = tf.losses.softmax_cross_entropy(tf.one_hot(label, NUM_CLASS), logits)\n",
    "loss = tf.losses.softmax_cross_entropy(label, logits)\n",
    "\n",
    "# Training op, the global step is critical here, make sure it matches the one used in pruning later\n",
    "# running this operation increments the global_step\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss, global_step=global_step)\n",
    "\n",
    "# Accuracy ops\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver(var_list={'conv1-1': 'Conv/weights:0'})\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./Model_Given\")\n",
    "    #print(b1.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(dataset,labels, batch_size):\n",
    "    for index, offset in enumerate(range(0, dataset.shape[0], batch_size)):\n",
    "        x_epoch, y_epoch = np.array(dataset[offset: offset + batch_size,:]), np.array(labels[offset: offset +  batch_size])\n",
    "    return x_epoch, y_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(dataset,labels, batch_size):\n",
    "    N = dataset.shape[0]\n",
    "    indices = np.random.randint(N, size=batch_size)\n",
    "    x_epoch = dataset[indices]\n",
    "    y_epoch = labels[indices]\n",
    "    return x_epoch, y_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 512 # Entire training set\n",
    "model_path_unpruned = \"Model_Saves/Unpruned.ckpt\"\n",
    "model_path_pruned = \"Model_Saves/Pruned.ckpt\"\n",
    "NUM_CLASS = 10\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Import dataset\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "batches = int(len(mnist.train.images) / batch_size)\n",
    "\n",
    "# Define Placeholders\n",
    "# image = tf.placeholder(tf.float32, [None, 28,28])\n",
    "# label = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "image = tf.placeholder(name='images', dtype=tf.float32, shape=[None, 28, 28, 1])\n",
    "label = tf.placeholder(name='fine_labels', dtype=tf.int32, shape=[None,10])\n",
    "\n",
    "# # Define the model\n",
    "# layer1 = layers.masked_fully_connected(image, 300)\n",
    "# layer2 = layers.masked_fully_connected(layer1, 300)\n",
    "# logits = layers.masked_fully_connected(layer2, 10)\n",
    "\n",
    "# Define the model\n",
    "# layer1 = layers.masked_conv2d(image, 300, kernel_size=2)\n",
    "# layer2 = layers.masked_conv2d(layer1, 300, kernel_size=2)\n",
    "# logits = layers.masked_fully_connected(layer2, 10)\n",
    "\n",
    "_=image\n",
    "_ = layers.masked_conv2d(_, 96, (3, 3), 1, 'SAME')\n",
    "_ = tf.layers.batch_normalization(_, name='norm1-1')\n",
    "_ = layers.masked_conv2d(_, 96, (3, 3), 1, 'SAME')\n",
    "_ = tf.layers.batch_normalization(_, name='norm1-2')\n",
    "_ = tf.layers.max_pooling2d(_, (3, 3), 2, 'SAME',name='pool1')\n",
    "_ = layers.masked_conv2d(_, 192, (3, 3), 1, 'SAME')\n",
    "_ = tf.layers.batch_normalization(_, name='norm2-1')\n",
    "_ = layers.masked_conv2d(_, 192, (3, 3), 1, 'SAME')\n",
    "_ = tf.layers.batch_normalization(_, name='norm2-2')\n",
    "_ = tf.layers.max_pooling2d(_, (3, 3), 2, 'SAME', name='pool2')\n",
    "_ = layers.masked_conv2d(_, 192, (3, 3), 1, 'VALID')\n",
    "_ = tf.layers.batch_normalization(_, name='norm3')\n",
    "_ = layers.masked_conv2d(_, 192, (1, 1), 1)\n",
    "_ = tf.layers.batch_normalization(_, name='norm4')\n",
    "_ = layers.masked_conv2d(_, 10, (1, 1), 1)\n",
    "_ = tf.layers.batch_normalization(_, name='norm5')\n",
    "_ = tf.layers.average_pooling2d(_, (5,5), 1, name='avg_pool')\n",
    "y = _\n",
    "logits = tf.reshape(y,[tf.shape(y)[0],10])\n",
    "\n",
    "\n",
    "# Create global step variable (needed for pruning)\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "reset_global_step_op = tf.assign(global_step, 0)\n",
    "\n",
    "# Loss function\n",
    "#loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=label))\n",
    "#loss = tf.losses.softmax_cross_entropy(tf.one_hot(label, NUM_CLASS), logits)\n",
    "loss = tf.losses.softmax_cross_entropy(label, logits)\n",
    "\n",
    "# Training op, the global step is critical here, make sure it matches the one used in pruning later\n",
    "# running this operation increments the global_step\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss, global_step=global_step)\n",
    "\n",
    "# Accuracy ops\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get, Print, and Edit Pruning Hyperparameters\n",
    "pruning_hparams = pruning.get_pruning_hparams()\n",
    "print(\"Pruning Hyperparameters:\", pruning_hparams)\n",
    "\n",
    "# Change hyperparameters to meet our needs\n",
    "pruning_hparams.begin_pruning_step = 0\n",
    "pruning_hparams.end_pruning_step = 250\n",
    "pruning_hparams.pruning_frequency = 1\n",
    "pruning_hparams.sparsity_function_end_step = 250\n",
    "pruning_hparams.target_sparsity = .5\n",
    "\n",
    "# Create a pruning object using the pruning specification, sparsity seems to have priority over the hparam\n",
    "p = pruning.Pruning(pruning_hparams, global_step=global_step) #sparsity=.5)\n",
    "prune_op = p.conditional_mask_update_op()\n",
    "\n",
    "# Create a saver for writing training checkpoints.\n",
    "saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    # Uncomment the following if you don't have a trained model yet\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Train the model before pruning (optional)\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            #print(batch_xs.shape)\n",
    "            batch_xs = batch_xs.reshape(-1,28,28,1)\n",
    "            sess.run(train_op, feed_dict={image: batch_xs, label: batch_ys})\n",
    "\n",
    "        # Calculate Test Accuracy every 10 epochs\n",
    "        if epoch % 1 == 0:\n",
    "            acc_print = 0\n",
    "            for batch in range(batches):\n",
    "                batch_xt, batch_yt = mnist.test.next_batch(batch_size)\n",
    "                #print(batch_xt.shape)\n",
    "                batch_xt = batch_xt.reshape(-1,28,28,1)\n",
    "                acc_print += sess.run(accuracy, feed_dict={image: batch_xt, label: batch_yt})\n",
    "            print(\"Un-pruned model step %d test accuracy %g\" % (epoch, acc_print/batches))\n",
    "        print(epoch)\n",
    "    \n",
    "    # Saves the model before pruning\n",
    "    saver.save(sess, model_path_unpruned)\n",
    "    \n",
    "    acc_print = sess.run(accuracy, feed_dict={image: mnist.test.images.reshape(-1,28,28,1), label: mnist.test.labels})\n",
    "    print(\"Pre-Pruning accuracy:\", acc_print)\n",
    "    print(\"Sparsity of layers (should be 0)\", sess.run(tf.contrib.model_pruning.get_weight_sparsity()))\n",
    "\n",
    "    \n",
    "\n",
    "    # Resets the session and restores the saved model\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    saver.restore(sess, model_path_unpruned)\n",
    "\n",
    "    # Reset the global step counter and begin pruning\n",
    "    sess.run(reset_global_step_op)\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            batch_xs = batch_xs.reshape(-1,28,28,1)\n",
    "            # Prune and retrain\n",
    "            sess.run(prune_op)\n",
    "            sess.run(train_op, feed_dict={image: batch_xs, label: batch_ys})\n",
    "\n",
    "        # Calculate Test Accuracy every 10 epochs\n",
    "        if epoch % 1 == 0:\n",
    "            acc_print = 0\n",
    "            for batch in range(batches):\n",
    "                batch_xt, batch_yt = mnist.test.next_batch(batch_size)\n",
    "                #print(batch_xt.shape)\n",
    "                batch_xt = batch_xt.reshape(-1,28,28,1)\n",
    "                acc_print += sess.run(accuracy, feed_dict={image: batch_xt, label: batch_yt})\n",
    "            \n",
    "            print(\"Pruned model step %d test accuracy %g\" % (epoch, acc_print/batches))\n",
    "            print(\"Weight sparsities:\", sess.run(tf.contrib.model_pruning.get_weight_sparsity()))\n",
    "        print(epoch) \n",
    "        \n",
    "           # acc_print = sess.run(accuracy, feed_dict={image: mnist.test.images, label: mnist.test.labels})\n",
    "\n",
    "    # Saves the model after pruning\n",
    "    saver.save(sess, model_path_pruned)\n",
    "\n",
    "    # Print final accuracy\n",
    "    #acc_print = sess.run(accuracy, feed_dict={image: mnist.test.images, label: mnist.test.labels})\n",
    "    print(\"Final accuracy:\", acc_print)\n",
    "    print(\"Final sparsity by layer (should be 0)\", sess.run(tf.contrib.model_pruning.get_weight_sparsity()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
