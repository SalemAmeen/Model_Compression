{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.model_pruning.python import pruning\n",
    "from tensorflow.contrib.model_pruning.python.layers import layers\n",
    "import keras\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "epochs_prune = 5\n",
    "batch_size = 250 # Entire training set\n",
    "model_path_unpruned = \"Model_Saves/Unpruned{}.ckpt\"\n",
    "model_path_pruned = \"Model_Saves/Pruned{}.ckpt\"\n",
    "NUM_CLASS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading numpy\n",
      "Loading numpy\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "#Load Dataset\n",
    "X_train0, y_train = prepare_dataset(data_dir, 'train')\n",
    "X_test0, y_test = prepare_dataset(data_dir, 'test')\n",
    "t = int(time.time())\n",
    "\n",
    "#Normalizing\n",
    "mean = np.mean(X_train0,axis=(0,1,2,3))\n",
    "std = np.std(X_train0,axis=(0,1,2,3))\n",
    "np.save('mean',mean)\n",
    "np.save('std',std)\n",
    "X_train = z_normalization(X_train0, mean, std)\n",
    "X_test = z_normalization(X_test0, mean, std)\n",
    "\n",
    "#Labels to binary\n",
    "y_train_binary = keras.utils.to_categorical(y_train,num_classes)\n",
    "y_test_binary = keras.utils.to_categorical(y_test,num_classes)\n",
    "\n",
    "batches = int(len(X_train) / batch_size)\n",
    "batches_test = int(len(X_test) / batch_size)\n",
    "print(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(dataset, labels, batch_size):\n",
    "    N = dataset.shape[0]\n",
    "    indices = np.random.randint(N, size=batch_size)\n",
    "    x_epoch = dataset[indices]\n",
    "    y_epoch = labels[indices]\n",
    "    return x_epoch, y_epoch\n",
    "\n",
    "def test_batch(dataset, labels, batch_size):\n",
    "    for index, offset in enumerate(range(0, dataset.shape[0], batch_size)):\n",
    "        x_epoch, y_epoch = np.array(dataset[offset: offset + batch_size,:]), np.array(labels[offset: offset +  batch_size])\n",
    "    return x_epoch, y_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "image = tf.placeholder(name='images', dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "label = tf.placeholder(name='fine_labels', dtype=tf.int32, shape=[None, 10])\n",
    "\n",
    "# # Define the model\n",
    "# layer1 = layers.masked_fully_connected(image, 300)\n",
    "# layer2 = layers.masked_fully_connected(layer1, 300)\n",
    "# logits = layers.masked_fully_connected(layer2, 10)\n",
    "\n",
    "# Define the model\n",
    "# layer1 = layers.masked_conv2d(image, 300, kernel_size=2)\n",
    "# layer2 = layers.masked_conv2d(layer1, 300, kernel_size=2)\n",
    "# logits = layers.masked_fully_connected(layer2, 10)\n",
    "\n",
    "_=image\n",
    "_ = layers.masked_conv2d(_, 96, (3, 3), 1, 'SAME')\n",
    "_ = tf.layers.batch_normalization(_, name='norm1-1')\n",
    "_ = layers.masked_conv2d(_, 96, (3, 3), 1, 'SAME')\n",
    "_ = tf.layers.batch_normalization(_, name='norm1-2')\n",
    "_ = tf.layers.max_pooling2d(_, (3, 3), 2, 'SAME',name='pool1')\n",
    "_ = layers.masked_conv2d(_, 192, (3, 3), 1, 'SAME')\n",
    "_ = tf.layers.batch_normalization(_, name='norm2-1')\n",
    "_ = layers.masked_conv2d(_, 192, (3, 3), 1, 'SAME')\n",
    "_ = tf.layers.batch_normalization(_, name='norm2-2')\n",
    "_ = tf.layers.max_pooling2d(_, (3, 3), 2, 'SAME', name='pool2')\n",
    "_ = layers.masked_conv2d(_, 192, (3, 3), 1, 'VALID')\n",
    "_ = tf.layers.batch_normalization(_, name='norm3')\n",
    "_ = layers.masked_conv2d(_, 192, (1, 1), 1)\n",
    "_ = tf.layers.batch_normalization(_, name='norm4')\n",
    "_ = layers.masked_conv2d(_, 10, (1, 1), 1)\n",
    "_ = tf.layers.batch_normalization(_, name='norm5')\n",
    "_ = tf.layers.average_pooling2d(_, (6,6), 1, name='avg_pool')\n",
    "y = _\n",
    "logits = tf.reshape(y,[tf.shape(y)[0],10])\n",
    "\n",
    "\n",
    "# Create global step variable (needed for pruning)\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "reset_global_step_op = tf.assign(global_step, 0)\n",
    "\n",
    "# Loss function\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(label, logits)\n",
    "\n",
    "# Training op, the global step is critical here, make sure it matches the one used in pruning later\n",
    "# running this operation increments the global_step\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss, global_step=global_step)\n",
    "# train_op = tf.train.GradientDescentOptimizer(\n",
    "#         learning_rate=0.001,\n",
    "#         # beta1=0.9,\n",
    "#         # beta2=0.999,\n",
    "#         # epsilon=1e-08,\n",
    "#         use_locking=False,\n",
    "#         name='GD'\n",
    "#     ).minimize(loss, global_step=global_step)\n",
    "\n",
    "# Accuracy ops\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "accuracy_5 = tf.reduce_mean(tf.cast(tf.nn.in_top_k(predictions=logits, targets=tf.argmax(label, 1), k=5), tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get, Print, and Edit Pruning Hyperparameters\n",
    "def set_prune_params(s):\n",
    "    pruning_hparams = pruning.get_pruning_hparams()\n",
    "    print(\"Pruning Hyperparameters:\", pruning_hparams)\n",
    "\n",
    "    # Change hyperparameters to meet our needs\n",
    "    pruning_hparams.begin_pruning_step = 0\n",
    "    pruning_hparams.end_pruning_step = 250\n",
    "    pruning_hparams.pruning_frequency = 1\n",
    "    pruning_hparams.sparsity_function_end_step = 250\n",
    "    pruning_hparams.target_sparsity = s\n",
    "\n",
    "    # Create a pruning object using the pruning specification, sparsity seems to have priority over the hparam\n",
    "    p = pruning.Pruning(pruning_hparams, global_step=global_step) #sparsity=.5)\n",
    "    prune_op = p.conditional_mask_update_op()\n",
    "    return prune_op\n",
    "\n",
    "# Create a saver for writing training checkpoints.\n",
    "saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Un-pruned model step 4 test accuracy 0.8021\n",
      "Un-pruned model step 4 test top5-accuracy 0.988\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Un-pruned model step 9 test accuracy 0.8214\n",
      "Un-pruned model step 9 test top5-accuracy 0.9916\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "Un-pruned model step 14 test accuracy 0.8305\n",
      "Un-pruned model step 14 test top5-accuracy 0.9921\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "Un-pruned model step 19 test accuracy 0.8279\n",
      "Un-pruned model step 19 test top5-accuracy 0.9912\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "Un-pruned model step 24 test accuracy 0.8288\n",
      "Un-pruned model step 24 test top5-accuracy 0.9915\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "Un-pruned model step 29 test accuracy 0.835\n",
      "Un-pruned model step 29 test top5-accuracy 0.9918\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "Un-pruned model step 34 test accuracy 0.829\n",
      "Un-pruned model step 34 test top5-accuracy 0.9913\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "Un-pruned model step 39 test accuracy 0.8252\n",
      "Un-pruned model step 39 test top5-accuracy 0.9904\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "Un-pruned model step 44 test accuracy 0.839\n",
      "Un-pruned model step 44 test top5-accuracy 0.9913\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "Un-pruned model step 49 test accuracy 0.832\n",
      "Un-pruned model step 49 test top5-accuracy 0.9883\n",
      "49\n",
      "Top-1 Pre-Pruning accuracy: 0.8320000022649765\n",
      "Top-5 Pre-Pruning accuracy: 0.988299997150898\n",
      "Sparsity of layers (should be 0) [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    # Uncomment the following if you don't have a trained model yet\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Train the model before pruning (optional)\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(batches):\n",
    "            batch_xs, batch_ys = sample_batch(X_train, y_train_binary, batch_size)\n",
    "            sess.run(train_op, feed_dict={image: batch_xs, label: batch_ys})\n",
    "\n",
    "        # Calculate Test Accuracy every 10 epochs\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            acc_print = 0\n",
    "            acc_print_5 = 0\n",
    "            for index, offset in enumerate(range(0, X_test.shape[0], batch_size)):\n",
    "                batch_xt = np.array(X_test[offset: offset + batch_size,:]) \n",
    "                batch_yt = np.array(y_test_binary[offset: offset +  batch_size])\n",
    "#             for batch in range(batches_test):\n",
    "#                 batch_xt, batch_yt = test_batch(X_test, y_test_binary, batch_size)\n",
    "                acc_print += sess.run(accuracy, feed_dict={image: batch_xt, label: batch_yt})\n",
    "                acc_print_5 += sess.run(accuracy_5, feed_dict={image: batch_xt, label: batch_yt})\n",
    "            print(\"Un-pruned model step %d test accuracy %g\" % (epoch, acc_print/batches_test))\n",
    "            print(\"Un-pruned model step %d test top5-accuracy %g\" % (epoch, acc_print_5/batches_test))\n",
    "            \n",
    "            # Saves the model before pruning\n",
    "            saver.save(sess, model_path_unpruned.format(epoch))\n",
    "        print(epoch)\n",
    "            \n",
    "        \n",
    "    print(\"Top-1 Pre-Pruning accuracy:\", acc_print/batches_test)\n",
    "    print(\"Top-5 Pre-Pruning accuracy:\", acc_print_5/batches_test)\n",
    "    print(\"Sparsity of layers (should be 0)\", sess.run(tf.contrib.model_pruning.get_weight_sparsity()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning Hyperparameters: [('begin_pruning_step', 0), ('block_height', 1), ('block_pooling_function', 'AVG'), ('block_width', 1), ('end_pruning_step', -1), ('initial_sparsity', 0), ('name', 'model_pruning'), ('nbins', 256), ('pruning_frequency', 10), ('sparsity_function_begin_step', 0), ('sparsity_function_end_step', 100), ('sparsity_function_exponent', 3), ('target_sparsity', 0.5), ('threshold_decay', 0.9), ('use_tpu', False), ('weight_sparsity_map', [''])]\n",
      "INFO:tensorflow:Updating masks.\n",
      "Target sparsity is 0.1\n",
      "INFO:tensorflow:Restoring parameters from Model_Saves/Unpruned44.ckpt\n",
      "Pruned model step 0 test accuracy 0.8367\n",
      "Pruned model step 0 test top5-accuracy 0.9912\n",
      "Weight sparsities: [0.09915123, 0.09343654, 0.087981045, 0.09233037, 0.09408155, 0.09423828, 0.096875]\n",
      "0\n",
      "Pruned model step 1 test accuracy 0.8413\n",
      "Pruned model step 1 test top5-accuracy 0.9902\n",
      "Weight sparsities: [0.099922836, 0.09938031, 0.097668305, 0.09804808, 0.09902163, 0.09901258, 0.09791667]\n",
      "1\n",
      "Pruned model step 2 test accuracy 0.8313\n",
      "Pruned model step 2 test top5-accuracy 0.993\n",
      "Weight sparsities: [0.099922836, 0.09938031, 0.097668305, 0.09804808, 0.09902163, 0.09901258, 0.09791667]\n",
      "2\n",
      "Pruned model step 3 test accuracy 0.8425\n",
      "Pruned model step 3 test top5-accuracy 0.9927\n",
      "Weight sparsities: [0.099922836, 0.09938031, 0.097668305, 0.09804808, 0.09902163, 0.09901258, 0.09791667]\n",
      "3\n",
      "Pruned model step 4 test accuracy 0.8372\n",
      "Pruned model step 4 test top5-accuracy 0.9911\n",
      "Weight sparsities: [0.099922836, 0.09938031, 0.097668305, 0.09804808, 0.09902163, 0.09901258, 0.09791667]\n",
      "4\n",
      "Final accuracy: 0.8371999993920326\n",
      "Final sparsity by layer [0.099922836, 0.09938031, 0.097668305, 0.09804808, 0.09902163, 0.09901258, 0.09791667]\n",
      "Pruning Hyperparameters: [('begin_pruning_step', 0), ('block_height', 1), ('block_pooling_function', 'AVG'), ('block_width', 1), ('end_pruning_step', -1), ('initial_sparsity', 0), ('name', 'model_pruning'), ('nbins', 256), ('pruning_frequency', 10), ('sparsity_function_begin_step', 0), ('sparsity_function_end_step', 100), ('sparsity_function_exponent', 3), ('target_sparsity', 0.5), ('threshold_decay', 0.9), ('use_tpu', False), ('weight_sparsity_map', [''])]\n",
      "INFO:tensorflow:Updating masks.\n",
      "Target sparsity is 0.2\n",
      "INFO:tensorflow:Restoring parameters from Model_Saves/Unpruned44.ckpt\n",
      "Pruned model step 0 test accuracy 0.8393\n",
      "Pruned model step 0 test top5-accuracy 0.9915\n",
      "Weight sparsities: [0.19791667, 0.19423948, 0.19021267, 0.19628002, 0.19140625, 0.19406468, 0.19583334]\n",
      "0\n",
      "Pruned model step 1 test accuracy 0.8464\n",
      "Pruned model step 1 test top5-accuracy 0.9922\n",
      "Weight sparsities: [0.19637346, 0.1992911, 0.1949267, 0.19326895, 0.19093002, 0.19908312, 0.1984375]\n",
      "1\n",
      "Pruned model step 2 test accuracy 0.843\n",
      "Pruned model step 2 test top5-accuracy 0.9908\n",
      "Weight sparsities: [0.19637346, 0.1992911, 0.1949267, 0.19326895, 0.19093002, 0.19908312, 0.1984375]\n",
      "2\n",
      "Pruned model step 3 test accuracy 0.8434\n",
      "Pruned model step 3 test top5-accuracy 0.9934\n",
      "Weight sparsities: [0.19637346, 0.1992911, 0.1949267, 0.19326895, 0.19093002, 0.19908312, 0.1984375]\n",
      "3\n",
      "Pruned model step 4 test accuracy 0.8244\n",
      "Pruned model step 4 test top5-accuracy 0.9902\n",
      "Weight sparsities: [0.19637346, 0.1992911, 0.1949267, 0.19326895, 0.19093002, 0.19908312, 0.1984375]\n",
      "4\n",
      "Final accuracy: 0.8243999972939491\n",
      "Final sparsity by layer [0.19637346, 0.1992911, 0.1949267, 0.19326895, 0.19093002, 0.19908312, 0.1984375]\n",
      "Pruning Hyperparameters: [('begin_pruning_step', 0), ('block_height', 1), ('block_pooling_function', 'AVG'), ('block_width', 1), ('end_pruning_step', -1), ('initial_sparsity', 0), ('name', 'model_pruning'), ('nbins', 256), ('pruning_frequency', 10), ('sparsity_function_begin_step', 0), ('sparsity_function_end_step', 100), ('sparsity_function_exponent', 3), ('target_sparsity', 0.5), ('threshold_decay', 0.9), ('use_tpu', False), ('weight_sparsity_map', [''])]\n",
      "INFO:tensorflow:Updating masks.\n",
      "Target sparsity is 0.3\n",
      "INFO:tensorflow:Restoring parameters from Model_Saves/Unpruned44.ckpt\n",
      "Pruned model step 0 test accuracy 0.8427\n",
      "Pruned model step 0 test top5-accuracy 0.9918\n",
      "Weight sparsities: [0.2951389, 0.29125676, 0.29590446, 0.29544634, 0.290883, 0.29329428, 0.29322916]\n",
      "0\n",
      "Pruned model step 1 test accuracy 0.8467\n",
      "Pruned model step 1 test top5-accuracy 0.9924\n",
      "Weight sparsities: [0.30015433, 0.29921392, 0.29512683, 0.2888063, 0.29862016, 0.2993978, 0.29895833]\n",
      "1\n",
      "Pruned model step 2 test accuracy 0.8497\n",
      "Pruned model step 2 test top5-accuracy 0.9919\n",
      "Weight sparsities: [0.30015433, 0.29921392, 0.29512683, 0.2888063, 0.29862016, 0.2993978, 0.29895833]\n",
      "2\n",
      "Pruned model step 3 test accuracy 0.8525\n",
      "Pruned model step 3 test top5-accuracy 0.9927\n",
      "Weight sparsities: [0.30015433, 0.29921392, 0.29512683, 0.2888063, 0.29862016, 0.2993978, 0.29895833]\n",
      "3\n",
      "Pruned model step 4 test accuracy 0.8528\n",
      "Pruned model step 4 test top5-accuracy 0.9933\n",
      "Weight sparsities: [0.30015433, 0.29921392, 0.29512683, 0.2888063, 0.29862016, 0.2993978, 0.29895833]\n",
      "4\n",
      "Final accuracy: 0.8527999997138977\n",
      "Final sparsity by layer [0.30015433, 0.29921392, 0.29512683, 0.2888063, 0.29862016, 0.2993978, 0.29895833]\n",
      "Pruning Hyperparameters: [('begin_pruning_step', 0), ('block_height', 1), ('block_pooling_function', 'AVG'), ('block_width', 1), ('end_pruning_step', -1), ('initial_sparsity', 0), ('name', 'model_pruning'), ('nbins', 256), ('pruning_frequency', 10), ('sparsity_function_begin_step', 0), ('sparsity_function_end_step', 100), ('sparsity_function_exponent', 3), ('target_sparsity', 0.5), ('threshold_decay', 0.9), ('use_tpu', False), ('weight_sparsity_map', [''])]\n",
      "INFO:tensorflow:Updating masks.\n",
      "Target sparsity is 0.4\n",
      "INFO:tensorflow:Restoring parameters from Model_Saves/Unpruned44.ckpt\n",
      "Pruned model step 0 test accuracy 0.8358\n",
      "Pruned model step 0 test top5-accuracy 0.9902\n",
      "Weight sparsities: [0.39621913, 0.3949653, 0.3952908, 0.3920145, 0.3949653, 0.3919271, 0.39114583]\n",
      "0\n",
      "Pruned model step 1 test accuracy 0.8471\n",
      "Pruned model step 1 test top5-accuracy 0.9933\n",
      "Weight sparsities: [0.40007716, 0.39151716, 0.3936813, 0.38966352, 0.39371744, 0.39979383, 0.39791667]\n",
      "1\n",
      "Pruned model step 2 test accuracy 0.8504\n",
      "Pruned model step 2 test top5-accuracy 0.9925\n",
      "Weight sparsities: [0.40007716, 0.39151716, 0.3936813, 0.38966352, 0.39371744, 0.39979383, 0.39791667]\n",
      "2\n",
      "Pruned model step 3 test accuracy 0.8496\n",
      "Pruned model step 3 test top5-accuracy 0.9929\n",
      "Weight sparsities: [0.40007716, 0.39151716, 0.3936813, 0.38966352, 0.39371744, 0.39979383, 0.39791667]\n",
      "3\n",
      "Pruned model step 4 test accuracy 0.8522\n",
      "Pruned model step 4 test top5-accuracy 0.9927\n",
      "Weight sparsities: [0.40007716, 0.39151716, 0.3936813, 0.38966352, 0.39371744, 0.39979383, 0.39791667]\n",
      "4\n",
      "Final accuracy: 0.8521999955177307\n",
      "Final sparsity by layer [0.40007716, 0.39151716, 0.3936813, 0.38966352, 0.39371744, 0.39979383, 0.39791667]\n",
      "Pruning Hyperparameters: [('begin_pruning_step', 0), ('block_height', 1), ('block_pooling_function', 'AVG'), ('block_width', 1), ('end_pruning_step', -1), ('initial_sparsity', 0), ('name', 'model_pruning'), ('nbins', 256), ('pruning_frequency', 10), ('sparsity_function_begin_step', 0), ('sparsity_function_end_step', 100), ('sparsity_function_exponent', 3), ('target_sparsity', 0.5), ('threshold_decay', 0.9), ('use_tpu', False), ('weight_sparsity_map', [''])]\n",
      "INFO:tensorflow:Updating masks.\n",
      "Target sparsity is 0.5\n",
      "INFO:tensorflow:Restoring parameters from Model_Saves/Unpruned44.ckpt\n",
      "Pruned model step 0 test accuracy 0.8402\n",
      "Pruned model step 0 test top5-accuracy 0.9925\n",
      "Weight sparsities: [0.49344134, 0.48954716, 0.48873335, 0.48952305, 0.48863992, 0.48909506, 0.48802084]\n",
      "0\n",
      "Pruned model step 1 test accuracy 0.8544\n",
      "Pruned model step 1 test top5-accuracy 0.9939\n",
      "Weight sparsities: [0.4992284, 0.49240452, 0.49746817, 0.49004146, 0.49528295, 0.49666342, 0.49791667]\n",
      "1\n",
      "Pruned model step 2 test accuracy 0.8575\n",
      "Pruned model step 2 test top5-accuracy 0.9932\n",
      "Weight sparsities: [0.4992284, 0.49240452, 0.49746817, 0.49004146, 0.49528295, 0.49666342, 0.49791667]\n",
      "2\n",
      "Pruned model step 3 test accuracy 0.855\n",
      "Pruned model step 3 test top5-accuracy 0.9931\n",
      "Weight sparsities: [0.4992284, 0.49240452, 0.49746817, 0.49004146, 0.49528295, 0.49666342, 0.49791667]\n",
      "3\n",
      "Pruned model step 4 test accuracy 0.8579\n",
      "Pruned model step 4 test top5-accuracy 0.9925\n",
      "Weight sparsities: [0.4992284, 0.49240452, 0.49746817, 0.49004146, 0.49528295, 0.49666342, 0.49791667]\n",
      "4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.8578999951481819\n",
      "Final sparsity by layer [0.4992284, 0.49240452, 0.49746817, 0.49004146, 0.49528295, 0.49666342, 0.49791667]\n",
      "Pruning Hyperparameters: [('begin_pruning_step', 0), ('block_height', 1), ('block_pooling_function', 'AVG'), ('block_width', 1), ('end_pruning_step', -1), ('initial_sparsity', 0), ('name', 'model_pruning'), ('nbins', 256), ('pruning_frequency', 10), ('sparsity_function_begin_step', 0), ('sparsity_function_end_step', 100), ('sparsity_function_exponent', 3), ('target_sparsity', 0.5), ('threshold_decay', 0.9), ('use_tpu', False), ('weight_sparsity_map', [''])]\n",
      "INFO:tensorflow:Updating masks.\n",
      "Target sparsity is 0.6\n",
      "INFO:tensorflow:Restoring parameters from Model_Saves/Unpruned44.ckpt\n",
      "Pruned model step 0 test accuracy 0.8366\n",
      "Pruned model step 0 test top5-accuracy 0.9906\n",
      "Weight sparsities: [0.58989197, 0.58575666, 0.5856542, 0.58273655, 0.58281493, 0.588406, 0.5859375]\n",
      "0\n",
      "Pruned model step 1 test accuracy 0.8486\n",
      "Pruned model step 1 test top5-accuracy 0.9921\n",
      "Weight sparsities: [0.5983796, 0.5969208, 0.59671587, 0.59560966, 0.5934456, 0.5996636, 0.59791666]\n",
      "1\n",
      "Pruned model step 2 test accuracy 0.8485\n",
      "Pruned model step 2 test top5-accuracy 0.9913\n",
      "Weight sparsities: [0.5983796, 0.5969208, 0.59671587, 0.59560966, 0.5934456, 0.5996636, 0.59791666]\n",
      "2\n",
      "Pruned model step 3 test accuracy 0.8508\n",
      "Pruned model step 3 test top5-accuracy 0.9927\n",
      "Weight sparsities: [0.5983796, 0.5969208, 0.59671587, 0.59560966, 0.5934456, 0.5996636, 0.59791666]\n",
      "3\n",
      "Pruned model step 4 test accuracy 0.8522\n",
      "Pruned model step 4 test top5-accuracy 0.9929\n",
      "Weight sparsities: [0.5983796, 0.5969208, 0.59671587, 0.59560966, 0.5934456, 0.5996636, 0.59791666]\n",
      "4\n",
      "Final accuracy: 0.8522000014781952\n",
      "Final sparsity by layer [0.5983796, 0.5969208, 0.59671587, 0.59560966, 0.5934456, 0.5996636, 0.59791666]\n",
      "Pruning Hyperparameters: [('begin_pruning_step', 0), ('block_height', 1), ('block_pooling_function', 'AVG'), ('block_width', 1), ('end_pruning_step', -1), ('initial_sparsity', 0), ('name', 'model_pruning'), ('nbins', 256), ('pruning_frequency', 10), ('sparsity_function_begin_step', 0), ('sparsity_function_end_step', 100), ('sparsity_function_exponent', 3), ('target_sparsity', 0.5), ('threshold_decay', 0.9), ('use_tpu', False), ('weight_sparsity_map', [''])]\n",
      "INFO:tensorflow:Updating masks.\n",
      "Target sparsity is 0.7\n",
      "INFO:tensorflow:Restoring parameters from Model_Saves/Unpruned44.ckpt\n",
      "Pruned model step 0 test accuracy 0.8174\n",
      "Pruned model step 0 test top5-accuracy 0.9885\n",
      "Weight sparsities: [0.6878858, 0.68776524, 0.68582416, 0.686864, 0.6838228, 0.6877984, 0.6875]\n",
      "0\n",
      "Pruned model step 1 test accuracy 0.8449\n",
      "Pruned model step 1 test top5-accuracy 0.9926\n",
      "Weight sparsities: [0.6998457, 0.6987365, 0.69926095, 0.6984532, 0.6961323, 0.6994629, 0.6984375]\n",
      "1\n",
      "Pruned model step 2 test accuracy 0.8434\n",
      "Pruned model step 2 test top5-accuracy 0.9924\n",
      "Weight sparsities: [0.6998457, 0.6987365, 0.69926095, 0.6984532, 0.6961323, 0.6994629, 0.6984375]\n",
      "2\n",
      "Pruned model step 3 test accuracy 0.8463\n",
      "Pruned model step 3 test top5-accuracy 0.9936\n",
      "Weight sparsities: [0.6998457, 0.6987365, 0.69926095, 0.6984532, 0.6961323, 0.6994629, 0.6984375]\n",
      "3\n",
      "Pruned model step 4 test accuracy 0.8477\n",
      "Pruned model step 4 test top5-accuracy 0.9921\n",
      "Weight sparsities: [0.6998457, 0.6987365, 0.69926095, 0.6984532, 0.6961323, 0.6994629, 0.6984375]\n",
      "4\n",
      "Final accuracy: 0.8476999968290329\n",
      "Final sparsity by layer [0.6998457, 0.6987365, 0.69926095, 0.6984532, 0.6961323, 0.6994629, 0.6984375]\n",
      "Pruning Hyperparameters: [('begin_pruning_step', 0), ('block_height', 1), ('block_pooling_function', 'AVG'), ('block_width', 1), ('end_pruning_step', -1), ('initial_sparsity', 0), ('name', 'model_pruning'), ('nbins', 256), ('pruning_frequency', 10), ('sparsity_function_begin_step', 0), ('sparsity_function_end_step', 100), ('sparsity_function_exponent', 3), ('target_sparsity', 0.5), ('threshold_decay', 0.9), ('use_tpu', False), ('weight_sparsity_map', [''])]\n",
      "INFO:tensorflow:Updating masks.\n",
      "Target sparsity is 0.8\n",
      "INFO:tensorflow:Restoring parameters from Model_Saves/Unpruned44.ckpt\n",
      "Pruned model step 0 test accuracy 0.8128\n",
      "Pruned model step 0 test top5-accuracy 0.9899\n",
      "Weight sparsities: [0.79012346, 0.78791714, 0.7866151, 0.7848428, 0.7841827, 0.786594, 0.78125]\n",
      "0\n",
      "Pruned model step 1 test accuracy 0.8334\n",
      "Pruned model step 1 test top5-accuracy 0.9918\n",
      "Weight sparsities: [0.80054015, 0.79814094, 0.79996145, 0.79969317, 0.7991657, 0.7995063, 0.7984375]\n",
      "1\n",
      "Pruned model step 2 test accuracy 0.8336\n",
      "Pruned model step 2 test top5-accuracy 0.9898\n",
      "Weight sparsities: [0.80054015, 0.79814094, 0.79996145, 0.79969317, 0.7991657, 0.7995063, 0.7984375]\n",
      "2\n",
      "Pruned model step 3 test accuracy 0.8341\n",
      "Pruned model step 3 test top5-accuracy 0.991\n",
      "Weight sparsities: [0.80054015, 0.79814094, 0.79996145, 0.79969317, 0.7991657, 0.7995063, 0.7984375]\n",
      "3\n",
      "Pruned model step 4 test accuracy 0.8392\n",
      "Pruned model step 4 test top5-accuracy 0.9925\n",
      "Weight sparsities: [0.80054015, 0.79814094, 0.79996145, 0.79969317, 0.7991657, 0.7995063, 0.7984375]\n",
      "4\n",
      "Final accuracy: 0.8392000004649163\n",
      "Final sparsity by layer [0.80054015, 0.79814094, 0.79996145, 0.79969317, 0.7991657, 0.7995063, 0.7984375]\n",
      "Pruning Hyperparameters: [('begin_pruning_step', 0), ('block_height', 1), ('block_pooling_function', 'AVG'), ('block_width', 1), ('end_pruning_step', -1), ('initial_sparsity', 0), ('name', 'model_pruning'), ('nbins', 256), ('pruning_frequency', 10), ('sparsity_function_begin_step', 0), ('sparsity_function_end_step', 100), ('sparsity_function_exponent', 3), ('target_sparsity', 0.5), ('threshold_decay', 0.9), ('use_tpu', False), ('weight_sparsity_map', [''])]\n",
      "INFO:tensorflow:Updating masks.\n",
      "Target sparsity is 0.9\n",
      "INFO:tensorflow:Restoring parameters from Model_Saves/Unpruned44.ckpt\n",
      "Pruned model step 0 test accuracy 0.7273\n",
      "Pruned model step 0 test top5-accuracy 0.9791\n",
      "Weight sparsities: [0.88695985, 0.88498265, 0.88468724, 0.8843316, 0.8841387, 0.8854709, 0.8833333]\n",
      "0\n",
      "Pruned model step 1 test accuracy 0.8086\n",
      "Pruned model step 1 test top5-accuracy 0.99\n",
      "Weight sparsities: [0.90123457, 0.90043885, 0.89983606, 0.90011334, 0.8995165, 0.8994412, 0.89947915]\n",
      "1\n",
      "Pruned model step 2 test accuracy 0.8121\n",
      "Pruned model step 2 test top5-accuracy 0.9909\n",
      "Weight sparsities: [0.90123457, 0.90043885, 0.89983606, 0.90011334, 0.8995165, 0.8994412, 0.89947915]\n",
      "2\n",
      "Pruned model step 3 test accuracy 0.8174\n",
      "Pruned model step 3 test top5-accuracy 0.9914\n",
      "Weight sparsities: [0.90123457, 0.90043885, 0.89983606, 0.90011334, 0.8995165, 0.8994412, 0.89947915]\n",
      "3\n",
      "Pruned model step 4 test accuracy 0.819\n",
      "Pruned model step 4 test top5-accuracy 0.9903\n",
      "Weight sparsities: [0.90123457, 0.90043885, 0.89983606, 0.90011334, 0.8995165, 0.8994412, 0.89947915]\n",
      "4\n",
      "Final accuracy: 0.8190000042319298\n",
      "Final sparsity by layer [0.90123457, 0.90043885, 0.89983606, 0.90011334, 0.8995165, 0.8994412, 0.89947915]\n",
      "Pruning Hyperparameters: [('begin_pruning_step', 0), ('block_height', 1), ('block_pooling_function', 'AVG'), ('block_width', 1), ('end_pruning_step', -1), ('initial_sparsity', 0), ('name', 'model_pruning'), ('nbins', 256), ('pruning_frequency', 10), ('sparsity_function_begin_step', 0), ('sparsity_function_end_step', 100), ('sparsity_function_exponent', 3), ('target_sparsity', 0.5), ('threshold_decay', 0.9), ('use_tpu', False), ('weight_sparsity_map', [''])]\n",
      "INFO:tensorflow:Updating masks.\n",
      "Target sparsity is 0.95\n",
      "INFO:tensorflow:Restoring parameters from Model_Saves/Unpruned44.ckpt\n",
      "Pruned model step 0 test accuracy 0.6073\n",
      "Pruned model step 0 test top5-accuracy 0.9571\n",
      "Weight sparsities: [0.9340278, 0.93487173, 0.9342388, 0.9342116, 0.9341604, 0.93511283, 0.9302083]\n",
      "0\n",
      "Pruned model step 1 test accuracy 0.7171\n",
      "Pruned model step 1 test top5-accuracy 0.9797\n",
      "Weight sparsities: [0.9506173, 0.95024353, 0.9499843, 0.9497884, 0.9498246, 0.9496528, 0.94947916]\n",
      "1\n",
      "Pruned model step 2 test accuracy 0.745\n",
      "Pruned model step 2 test top5-accuracy 0.9851\n",
      "Weight sparsities: [0.9506173, 0.95024353, 0.9499843, 0.9497884, 0.9498246, 0.9496528, 0.94947916]\n",
      "2\n",
      "Pruned model step 3 test accuracy 0.7637\n",
      "Pruned model step 3 test top5-accuracy 0.9872\n",
      "Weight sparsities: [0.9506173, 0.95024353, 0.9499843, 0.9497884, 0.9498246, 0.9496528, 0.94947916]\n",
      "3\n",
      "Pruned model step 4 test accuracy 0.7703\n",
      "Pruned model step 4 test top5-accuracy 0.9879\n",
      "Weight sparsities: [0.9506173, 0.95024353, 0.9499843, 0.9497884, 0.9498246, 0.9496528, 0.94947916]\n",
      "4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.7702999994158745\n",
      "Final sparsity by layer [0.9506173, 0.95024353, 0.9499843, 0.9497884, 0.9498246, 0.9496528, 0.94947916]\n",
      "Pruning Hyperparameters: [('begin_pruning_step', 0), ('block_height', 1), ('block_pooling_function', 'AVG'), ('block_width', 1), ('end_pruning_step', -1), ('initial_sparsity', 0), ('name', 'model_pruning'), ('nbins', 256), ('pruning_frequency', 10), ('sparsity_function_begin_step', 0), ('sparsity_function_end_step', 100), ('sparsity_function_exponent', 3), ('target_sparsity', 0.5), ('threshold_decay', 0.9), ('use_tpu', False), ('weight_sparsity_map', [''])]\n",
      "INFO:tensorflow:Updating masks.\n",
      "Target sparsity is 0.99\n",
      "INFO:tensorflow:Restoring parameters from Model_Saves/Unpruned44.ckpt\n",
      "Pruned model step 0 test accuracy 0.3692\n",
      "Pruned model step 0 test top5-accuracy 0.8765\n",
      "Weight sparsities: [0.97492284, 0.97480226, 0.9752544, 0.97541416, 0.9749982, 0.97488064, 0.9703125]\n",
      "0\n",
      "Pruned model step 1 test accuracy 0.3903\n",
      "Pruned model step 1 test top5-accuracy 0.8881\n",
      "Weight sparsities: [0.98996913, 0.9896798, 0.9894628, 0.98956525, 0.9893181, 0.9895562, 0.9890625]\n",
      "1\n",
      "Pruned model step 2 test accuracy 0.4359\n",
      "Pruned model step 2 test top5-accuracy 0.9064\n",
      "Weight sparsities: [0.98996913, 0.9896798, 0.9894628, 0.98956525, 0.9893181, 0.9895562, 0.9890625]\n",
      "2\n",
      "Pruned model step 3 test accuracy 0.4584\n",
      "Pruned model step 3 test top5-accuracy 0.9204\n",
      "Weight sparsities: [0.98996913, 0.9896798, 0.9894628, 0.98956525, 0.9893181, 0.9895562, 0.9890625]\n",
      "3\n",
      "Pruned model step 4 test accuracy 0.4782\n",
      "Pruned model step 4 test top5-accuracy 0.9276\n",
      "Weight sparsities: [0.98996913, 0.9896798, 0.9894628, 0.98956525, 0.9893181, 0.9895562, 0.9890625]\n",
      "4\n",
      "Final accuracy: 0.47820000201463697\n",
      "Final sparsity by layer [0.98996913, 0.9896798, 0.9894628, 0.98956525, 0.9893181, 0.9895562, 0.9890625]\n"
     ]
    }
   ],
   "source": [
    "for s in sparsity:\n",
    "    prune_op = set_prune_params(s)\n",
    "    print('Target sparsity is {}'.format(s))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Resets the session and restores the saved model\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, model_path_unpruned.format(44))\n",
    "\n",
    "        # Reset the global step counter and begin pruning\n",
    "        sess.run(reset_global_step_op)\n",
    "        for epoch in range(epochs_prune):\n",
    "            for batch in range(batches):\n",
    "                batch_xs, batch_ys = sample_batch(X_train, y_train_binary, batch_size)\n",
    "                # Prune and retrain\n",
    "                sess.run(prune_op)\n",
    "                sess.run(train_op, feed_dict={image: batch_xs, label: batch_ys})\n",
    "\n",
    "    #         # Calculate Test Accuracy every 10 epochs\n",
    "    #         if epoch % 1 == 0:\n",
    "    #             acc_print = 0\n",
    "    #             acc_print_5 = 0\n",
    "    #             for batch in range(batches):\n",
    "    #                 batch_xt, batch_yt = sample_batch(X_test, y_test_binary, batch_size)\n",
    "    #                 acc_print += sess.run(accuracy, feed_dict={image: batch_xt, label: batch_yt})\n",
    "            # Calculate Test Accuracy every 10 epochs\n",
    "            if (epoch+1) % 1 == 0:\n",
    "                acc_print = 0\n",
    "                acc_print_5 = 0\n",
    "                for index, offset in enumerate(range(0, X_test.shape[0], batch_size)):\n",
    "                    batch_xt = np.array(X_test[offset: offset + batch_size,:]) \n",
    "                    batch_yt = np.array(y_test_binary[offset: offset +  batch_size])\n",
    "    #             for batch in range(batches_test):\n",
    "    #                 batch_xt, batch_yt = test_batch(X_test, y_test_binary, batch_size)\n",
    "                    acc_print += sess.run(accuracy, feed_dict={image: batch_xt, label: batch_yt})\n",
    "                    acc_print_5 += sess.run(accuracy_5, feed_dict={image: batch_xt, label: batch_yt})\n",
    "\n",
    "                print(\"Pruned model step %d test accuracy %g\" % (epoch, acc_print/batches_test))\n",
    "                print(\"Pruned model step %d test top5-accuracy %g\" % (epoch, acc_print_5/batches_test))\n",
    "                print(\"Weight sparsities:\", sess.run(tf.contrib.model_pruning.get_weight_sparsity()))\n",
    "            print(epoch) \n",
    "\n",
    "               # acc_print = sess.run(accuracy, feed_dict={image: mnist.test.images, label: mnist.test.labels})\n",
    "\n",
    "        # Saves the model after pruning\n",
    "        saver.save(sess, model_path_pruned.format(s))\n",
    "\n",
    "        # Print final accuracy\n",
    "        #acc_print = sess.run(accuracy, feed_dict={image: mnist.test.images, label: mnist.test.labels})\n",
    "        print(\"Final accuracy:\", acc_print/batches_test)\n",
    "        print(\"Final sparsity by layer\", sess.run(tf.contrib.model_pruning.get_weight_sparsity()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv/biases (DT_FLOAT) [96]\n",
      "Conv/biases/Adam (DT_FLOAT) [96]\n",
      "Conv/biases/Adam_1 (DT_FLOAT) [96]\n",
      "Conv/mask (DT_FLOAT) [3,3,3,96]\n",
      "Conv/threshold (DT_FLOAT) []\n",
      "Conv/weights (DT_FLOAT) [3,3,3,96]\n",
      "Conv/weights/Adam (DT_FLOAT) [3,3,3,96]\n",
      "Conv/weights/Adam_1 (DT_FLOAT) [3,3,3,96]\n",
      "Conv_1/biases (DT_FLOAT) [96]\n",
      "Conv_1/biases/Adam (DT_FLOAT) [96]\n",
      "Conv_1/biases/Adam_1 (DT_FLOAT) [96]\n",
      "Conv_1/mask (DT_FLOAT) [3,3,96,96]\n",
      "Conv_1/threshold (DT_FLOAT) []\n",
      "Conv_1/weights (DT_FLOAT) [3,3,96,96]\n",
      "Conv_1/weights/Adam (DT_FLOAT) [3,3,96,96]\n",
      "Conv_1/weights/Adam_1 (DT_FLOAT) [3,3,96,96]\n",
      "Conv_2/biases (DT_FLOAT) [192]\n",
      "Conv_2/biases/Adam (DT_FLOAT) [192]\n",
      "Conv_2/biases/Adam_1 (DT_FLOAT) [192]\n",
      "Conv_2/mask (DT_FLOAT) [3,3,96,192]\n",
      "Conv_2/threshold (DT_FLOAT) []\n",
      "Conv_2/weights (DT_FLOAT) [3,3,96,192]\n",
      "Conv_2/weights/Adam (DT_FLOAT) [3,3,96,192]\n",
      "Conv_2/weights/Adam_1 (DT_FLOAT) [3,3,96,192]\n",
      "Conv_3/biases (DT_FLOAT) [192]\n",
      "Conv_3/biases/Adam (DT_FLOAT) [192]\n",
      "Conv_3/biases/Adam_1 (DT_FLOAT) [192]\n",
      "Conv_3/mask (DT_FLOAT) [3,3,192,192]\n",
      "Conv_3/threshold (DT_FLOAT) []\n",
      "Conv_3/weights (DT_FLOAT) [3,3,192,192]\n",
      "Conv_3/weights/Adam (DT_FLOAT) [3,3,192,192]\n",
      "Conv_3/weights/Adam_1 (DT_FLOAT) [3,3,192,192]\n",
      "Conv_4/biases (DT_FLOAT) [192]\n",
      "Conv_4/biases/Adam (DT_FLOAT) [192]\n",
      "Conv_4/biases/Adam_1 (DT_FLOAT) [192]\n",
      "Conv_4/mask (DT_FLOAT) [3,3,192,192]\n",
      "Conv_4/threshold (DT_FLOAT) []\n",
      "Conv_4/weights (DT_FLOAT) [3,3,192,192]\n",
      "Conv_4/weights/Adam (DT_FLOAT) [3,3,192,192]\n",
      "Conv_4/weights/Adam_1 (DT_FLOAT) [3,3,192,192]\n",
      "Conv_5/biases (DT_FLOAT) [192]\n",
      "Conv_5/biases/Adam (DT_FLOAT) [192]\n",
      "Conv_5/biases/Adam_1 (DT_FLOAT) [192]\n",
      "Conv_5/mask (DT_FLOAT) [1,1,192,192]\n",
      "Conv_5/threshold (DT_FLOAT) []\n",
      "Conv_5/weights (DT_FLOAT) [1,1,192,192]\n",
      "Conv_5/weights/Adam (DT_FLOAT) [1,1,192,192]\n",
      "Conv_5/weights/Adam_1 (DT_FLOAT) [1,1,192,192]\n",
      "Conv_6/biases (DT_FLOAT) [10]\n",
      "Conv_6/biases/Adam (DT_FLOAT) [10]\n",
      "Conv_6/biases/Adam_1 (DT_FLOAT) [10]\n",
      "Conv_6/mask (DT_FLOAT) [1,1,192,10]\n",
      "Conv_6/threshold (DT_FLOAT) []\n",
      "Conv_6/weights (DT_FLOAT) [1,1,192,10]\n",
      "Conv_6/weights/Adam (DT_FLOAT) [1,1,192,10]\n",
      "Conv_6/weights/Adam_1 (DT_FLOAT) [1,1,192,10]\n",
      "beta1_power (DT_FLOAT) []\n",
      "beta2_power (DT_FLOAT) []\n",
      "global_step (DT_INT64) []\n",
      "model_pruning/last_mask_update_step (DT_INT32) []\n",
      "norm1-1/beta (DT_FLOAT) [96]\n",
      "norm1-1/beta/Adam (DT_FLOAT) [96]\n",
      "norm1-1/beta/Adam_1 (DT_FLOAT) [96]\n",
      "norm1-1/gamma (DT_FLOAT) [96]\n",
      "norm1-1/gamma/Adam (DT_FLOAT) [96]\n",
      "norm1-1/gamma/Adam_1 (DT_FLOAT) [96]\n",
      "norm1-1/moving_mean (DT_FLOAT) [96]\n",
      "norm1-1/moving_variance (DT_FLOAT) [96]\n",
      "norm1-2/beta (DT_FLOAT) [96]\n",
      "norm1-2/beta/Adam (DT_FLOAT) [96]\n",
      "norm1-2/beta/Adam_1 (DT_FLOAT) [96]\n",
      "norm1-2/gamma (DT_FLOAT) [96]\n",
      "norm1-2/gamma/Adam (DT_FLOAT) [96]\n",
      "norm1-2/gamma/Adam_1 (DT_FLOAT) [96]\n",
      "norm1-2/moving_mean (DT_FLOAT) [96]\n",
      "norm1-2/moving_variance (DT_FLOAT) [96]\n",
      "norm2-1/beta (DT_FLOAT) [192]\n",
      "norm2-1/beta/Adam (DT_FLOAT) [192]\n",
      "norm2-1/beta/Adam_1 (DT_FLOAT) [192]\n",
      "norm2-1/gamma (DT_FLOAT) [192]\n",
      "norm2-1/gamma/Adam (DT_FLOAT) [192]\n",
      "norm2-1/gamma/Adam_1 (DT_FLOAT) [192]\n",
      "norm2-1/moving_mean (DT_FLOAT) [192]\n",
      "norm2-1/moving_variance (DT_FLOAT) [192]\n",
      "norm2-2/beta (DT_FLOAT) [192]\n",
      "norm2-2/beta/Adam (DT_FLOAT) [192]\n",
      "norm2-2/beta/Adam_1 (DT_FLOAT) [192]\n",
      "norm2-2/gamma (DT_FLOAT) [192]\n",
      "norm2-2/gamma/Adam (DT_FLOAT) [192]\n",
      "norm2-2/gamma/Adam_1 (DT_FLOAT) [192]\n",
      "norm2-2/moving_mean (DT_FLOAT) [192]\n",
      "norm2-2/moving_variance (DT_FLOAT) [192]\n",
      "norm3/beta (DT_FLOAT) [192]\n",
      "norm3/beta/Adam (DT_FLOAT) [192]\n",
      "norm3/beta/Adam_1 (DT_FLOAT) [192]\n",
      "norm3/gamma (DT_FLOAT) [192]\n",
      "norm3/gamma/Adam (DT_FLOAT) [192]\n",
      "norm3/gamma/Adam_1 (DT_FLOAT) [192]\n",
      "norm3/moving_mean (DT_FLOAT) [192]\n",
      "norm3/moving_variance (DT_FLOAT) [192]\n",
      "norm4/beta (DT_FLOAT) [192]\n",
      "norm4/beta/Adam (DT_FLOAT) [192]\n",
      "norm4/beta/Adam_1 (DT_FLOAT) [192]\n",
      "norm4/gamma (DT_FLOAT) [192]\n",
      "norm4/gamma/Adam (DT_FLOAT) [192]\n",
      "norm4/gamma/Adam_1 (DT_FLOAT) [192]\n",
      "norm4/moving_mean (DT_FLOAT) [192]\n",
      "norm4/moving_variance (DT_FLOAT) [192]\n",
      "norm5/beta (DT_FLOAT) [10]\n",
      "norm5/beta/Adam (DT_FLOAT) [10]\n",
      "norm5/beta/Adam_1 (DT_FLOAT) [10]\n",
      "norm5/gamma (DT_FLOAT) [10]\n",
      "norm5/gamma/Adam (DT_FLOAT) [10]\n",
      "norm5/gamma/Adam_1 (DT_FLOAT) [10]\n",
      "norm5/moving_mean (DT_FLOAT) [10]\n",
      "norm5/moving_variance (DT_FLOAT) [10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "print_tensors_in_checkpoint_file('Model_Saves/Unpruned44.ckpt', \n",
    "                                 all_tensors=False, all_tensor_names=False, tensor_name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_name:  Conv/weights\n",
      "[[[[ 0.04379602 -0.05505071 -0.08778767 ...  0.03066595 -0.04357576\n",
      "    -0.05353526]\n",
      "   [ 0.0070363   0.04169472 -0.0729975  ...  0.08931225 -0.06831206\n",
      "     0.10006074]\n",
      "   [ 0.12743428  0.07768662  0.0874206  ... -0.02181112 -0.04949465\n",
      "     0.09114436]]\n",
      "\n",
      "  [[-0.0539235  -0.03097104  0.01627925 ...  0.05563014 -0.09813255\n",
      "     0.1472321 ]\n",
      "   [ 0.00965081  0.02887369  0.01724323 ... -0.02030833 -0.11718856\n",
      "     0.07390618]\n",
      "   [ 0.08303585 -0.03235126  0.12467189 ... -0.1112793   0.02824809\n",
      "     0.08702562]]\n",
      "\n",
      "  [[-0.15823641 -0.06146544  0.00619495 ...  0.031168    0.04163508\n",
      "     0.13076901]\n",
      "   [-0.11911135  0.11352054 -0.05747633 ...  0.02983151 -0.07704147\n",
      "     0.10038877]\n",
      "   [-0.01146747  0.01727173  0.14272916 ...  0.06176776  0.0394776\n",
      "     0.03551508]]]\n",
      "\n",
      "\n",
      " [[[ 0.04224705  0.04160998 -0.02847889 ... -0.11748441 -0.06114323\n",
      "     0.15397218]\n",
      "   [ 0.06881077  0.05409041 -0.01212053 ... -0.13167563 -0.11558327\n",
      "    -0.02307469]\n",
      "   [-0.05883789 -0.02132129  0.03006083 ... -0.02524436  0.06732172\n",
      "    -0.00289567]]\n",
      "\n",
      "  [[ 0.07876228  0.10698162 -0.12561028 ...  0.08565594  0.1253728\n",
      "     0.0470198 ]\n",
      "   [ 0.1985988  -0.00622499 -0.04323286 ...  0.02728471 -0.00297842\n",
      "    -0.15220763]\n",
      "   [-0.01760605 -0.08169599  0.15862353 ... -0.09506826  0.0173293\n",
      "    -0.04217817]]\n",
      "\n",
      "  [[-0.10695773 -0.00214006 -0.08246107 ...  0.13150811  0.10040068\n",
      "    -0.07409123]\n",
      "   [ 0.09864505 -0.04230336  0.06953349 ...  0.10216331  0.06604917\n",
      "    -0.13356365]\n",
      "   [-0.09721622 -0.06350318  0.12325836 ...  0.02544441  0.04404116\n",
      "     0.05969787]]]\n",
      "\n",
      "\n",
      " [[[ 0.0329093   0.04552538 -0.10530369 ... -0.15019763 -0.1195146\n",
      "     0.07240649]\n",
      "   [-0.08620835 -0.11091778  0.06651576 ... -0.10444299 -0.13314609\n",
      "    -0.04229454]\n",
      "   [-0.13646819 -0.02414819  0.08056138 ... -0.03189154  0.01285567\n",
      "    -0.09452968]]\n",
      "\n",
      "  [[ 0.02472244  0.11727002 -0.11334747 ... -0.10115425  0.08081426\n",
      "    -0.10845968]\n",
      "   [ 0.12187478 -0.05382066 -0.00045788 ... -0.00411788 -0.05068187\n",
      "    -0.15383768]\n",
      "   [-0.12128572 -0.10150263  0.02516751 ... -0.02087223 -0.00838061\n",
      "    -0.06531229]]\n",
      "\n",
      "  [[-0.02551132  0.1330704  -0.05013803 ...  0.08113231  0.12081164\n",
      "    -0.11627962]\n",
      "   [ 0.08274572  0.01457939 -0.10266189 ...  0.04509542  0.02342885\n",
      "    -0.09300383]\n",
      "   [-0.0547647  -0.01599528 -0.09076149 ...  0.18447869 -0.04973695\n",
      "     0.02997721]]]]\n"
     ]
    }
   ],
   "source": [
    "print_tensors_in_checkpoint_file('Model_Saves/Unpruned44.ckpt', \n",
    "                                 all_tensors=False, all_tensor_names=False, tensor_name='Conv/weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess1:\n",
    "    sess1.run(\"Conv/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Conv/weights:0' shape=(3, 3, 3, 96) dtype=float32_ref>, <tf.Variable 'Conv/biases:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'norm1-1/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'norm1-1/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'Conv_1/weights:0' shape=(3, 3, 96, 96) dtype=float32_ref>, <tf.Variable 'Conv_1/biases:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'norm1-2/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'norm1-2/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'Conv_2/weights:0' shape=(3, 3, 96, 192) dtype=float32_ref>, <tf.Variable 'Conv_2/biases:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'norm2-1/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'norm2-1/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'Conv_3/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'Conv_3/biases:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'norm2-2/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'norm2-2/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'Conv_4/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'Conv_4/biases:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'norm3/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'norm3/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'Conv_5/weights:0' shape=(1, 1, 192, 192) dtype=float32_ref>, <tf.Variable 'Conv_5/biases:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'norm4/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'norm4/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'Conv_6/weights:0' shape=(1, 1, 192, 10) dtype=float32_ref>, <tf.Variable 'Conv_6/biases:0' shape=(10,) dtype=float32_ref>, <tf.Variable 'norm5/gamma:0' shape=(10,) dtype=float32_ref>, <tf.Variable 'norm5/beta:0' shape=(10,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "vars = tf.trainable_variables()\n",
    "print(vars) #some infos about variables...\n",
    "# with tf.Session() as sess1:\n",
    "#     vars_vals = sess1.run(vars)\n",
    "# for var, val in zip(vars, vars_vals):\n",
    "#     print(\"var: {}, value: {}\".format(var.name, val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Model_Saves/Pruned0.9.ckpt\n"
     ]
    }
   ],
   "source": [
    " with tf.Session() as sess:\n",
    "        # Resets the session and restores the saved model\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #saver.restore(sess, model_path_unpruned.format(44))\n",
    "        saver.restore(sess, model_path_pruned.format(0.9))\n",
    "        # get the graph\n",
    "        g = tf.get_default_graph()\n",
    "        w1 = g.get_tensor_by_name('Conv_2/threshold:0')\n",
    "        x = sess.run(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16020398"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training params: 0.95667M\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    shape = variable.get_shape()\n",
    "    variable_parametes = 1\n",
    "    for dim in shape:\n",
    "        variable_parametes *= dim.value\n",
    "    total_parameters += variable_parametes\n",
    "print(\"Total training params: %.5fM\" % (total_parameters / 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Model_Saves/Pruned0.9.ckpt\n",
      "Conv/weights:0 [[[[ 0.04386877 -0.05370045 -0.08818778 ...  0.03163777 -0.03516616\n",
      "    -0.05234836]\n",
      "   [ 0.00367678  0.03967545 -0.07397655 ...  0.08231409 -0.06926255\n",
      "     0.09879471]\n",
      "   [ 0.12032126  0.06689382  0.08355619 ... -0.02391103 -0.04284841\n",
      "     0.09426026]]\n",
      "\n",
      "  [[-0.0479385  -0.02921759  0.01969598 ...  0.05949609 -0.08868785\n",
      "     0.24975243]\n",
      "   [ 0.00721393  0.03141775  0.01831823 ... -0.01895187 -0.12521599\n",
      "     0.07258015]\n",
      "   [ 0.07973339 -0.03227412  0.14145286 ... -0.10893171  0.03382431\n",
      "     0.09231466]]\n",
      "\n",
      "  [[-0.14586031 -0.06017137  0.00895014 ...  0.03246674  0.05327276\n",
      "     0.13533467]\n",
      "   [-0.1040989   0.0871282  -0.05544589 ...  0.03138801 -0.06712543\n",
      "     0.11880873]\n",
      "   [-0.01569531  0.01624121  0.19445866 ...  0.06390522  0.04735205\n",
      "     0.03824019]]]\n",
      "\n",
      "\n",
      " [[[ 0.0377103   0.03618093 -0.02557194 ... -0.10170904 -0.05498583\n",
      "     0.1387983 ]\n",
      "   [ 0.06660452  0.05048921 -0.01265561 ... -0.14740188 -0.13671376\n",
      "    -0.02056931]\n",
      "   [-0.05851484 -0.02288001  0.02839403 ... -0.02615901  0.0668153\n",
      "    -0.00557155]]\n",
      "\n",
      "  [[ 0.0902836   0.08836722 -0.13987467 ...  0.09357996  0.13407199\n",
      "     0.04315119]\n",
      "   [ 0.20585456 -0.00934359 -0.04571726 ...  0.02690574 -0.00362529\n",
      "    -0.1468862 ]\n",
      "   [-0.02218138 -0.10891823  0.16469441 ... -0.0952581   0.01803652\n",
      "    -0.03779655]]\n",
      "\n",
      "  [[-0.09599394 -0.0026225  -0.07989328 ...  0.13458495  0.13432097\n",
      "    -0.07562391]\n",
      "   [ 0.10290078 -0.04528503  0.07092296 ...  0.10950255  0.07137229\n",
      "    -0.12945281]\n",
      "   [-0.09511426 -0.07437549  0.11979122 ...  0.02254929  0.04914832\n",
      "     0.06200177]]]\n",
      "\n",
      "\n",
      " [[[ 0.03158172  0.03963684 -0.12507054 ... -0.1760675  -0.11994973\n",
      "     0.06433712]\n",
      "   [-0.07953543 -0.11107132  0.07122111 ... -0.10496581 -0.1491105\n",
      "    -0.04096564]\n",
      "   [-0.14170028 -0.02580853  0.07980373 ... -0.03763156  0.01619838\n",
      "    -0.09407837]]\n",
      "\n",
      "  [[ 0.02569459  0.11860336 -0.13102281 ... -0.08542112  0.08577511\n",
      "    -0.11710266]\n",
      "   [ 0.1351155  -0.05598096  0.0002549  ... -0.00327712 -0.041344\n",
      "    -0.2060313 ]\n",
      "   [-0.12646763 -0.12356912  0.02943296 ... -0.02199106 -0.00648395\n",
      "    -0.06396688]]\n",
      "\n",
      "  [[-0.02250835  0.14443448 -0.05000145 ...  0.08396965  0.14978974\n",
      "    -0.11476874]\n",
      "   [ 0.08715121  0.01395829 -0.10957571 ...  0.04377338  0.0270714\n",
      "    -0.08980936]\n",
      "   [-0.0542124  -0.01730571 -0.08825255 ...  0.1362844  -0.04059572\n",
      "     0.03229   ]]]]\n",
      "Conv/biases:0 [ 0.04132941  0.0216579   0.06954278 -0.12475366 -0.03955756 -0.00379692\n",
      "  0.06564905  0.02931145  0.10584282 -0.06829056 -0.04097177 -0.13820048\n",
      "  0.13749471  0.06952131  0.09155582  0.04853277  0.07598454 -0.05084312\n",
      "  0.00234683 -0.04521099 -0.03017512  0.15148659 -0.11956917  0.01812517\n",
      "  0.09317694  0.11461848  0.02559447  0.08065504  0.09961303 -0.10007001\n",
      "  0.12705614  0.06774366  0.09131693  0.0863357   0.08119015  0.0134912\n",
      "  0.2194426  -0.00250593 -0.18356927  0.02128625  0.08117871 -0.11313879\n",
      "  0.07888    -0.07841641  0.00793757  0.09871538  0.12794825  0.10060697\n",
      "  0.1077498  -0.14433889  0.149648    0.02778227  0.02357396  0.01442618\n",
      " -0.00601369  0.05265502  0.04330108 -0.09424918 -0.02357237  0.10768028\n",
      " -0.03886348  0.08262824  0.09655096 -0.13384     0.01239305  0.20801571\n",
      " -0.01770824 -0.07913121  0.11009477  0.04163113  0.01940331 -0.06267072\n",
      " -0.16063847  0.04646458  0.13867654  0.10909021  0.16728453 -0.01647968\n",
      "  0.08282919  0.07327047 -0.18321335  0.04456148  0.07659692 -0.16666633\n",
      "  0.05518804 -0.00413407 -0.01519298  0.04571674 -0.12544662  0.07673255\n",
      "  0.17213057 -0.05756523 -0.2272128   0.13513191 -0.08314537 -0.02487775]\n",
      "norm1-1/gamma:0 [0.9371419  0.7828097  0.9540145  0.8058595  1.0982449  0.99886286\n",
      " 1.1945319  0.8095948  1.2889712  0.8871219  0.8477813  0.7902665\n",
      " 1.2539903  1.0202291  1.101581   1.1519592  0.9558853  0.96093285\n",
      " 1.0855117  1.1394564  0.98804826 1.2823923  0.94089377 1.2079105\n",
      " 1.0404772  1.0323067  0.96713686 1.1622905  1.1381916  0.896045\n",
      " 1.2885875  1.0299886  1.2402973  1.250995   1.2135386  0.89042807\n",
      " 1.5891677  0.9340452  0.87083685 0.8296472  1.1963931  0.86158085\n",
      " 1.1416341  0.75459844 0.88152534 0.9541231  1.1803298  1.2020432\n",
      " 1.2528142  0.9153632  1.1686838  1.0477731  1.0258536  1.3602065\n",
      " 1.0927943  1.0516169  1.0852492  0.8904374  0.8994367  1.3111652\n",
      " 0.9443143  1.1642605  1.272512   1.1497973  1.0411379  1.3765186\n",
      " 0.79233015 0.992603   1.1999453  0.9321182  1.199901   0.9150591\n",
      " 0.95877045 1.1314304  1.3671143  1.2579798  1.4568651  1.1614859\n",
      " 1.2449971  1.1027966  0.83471245 1.0510875  1.2562356  1.1192049\n",
      " 1.0116268  1.1119896  0.92770386 0.87385833 1.0059897  1.0063728\n",
      " 1.1502233  0.7594981  0.80116796 0.89266884 0.93166435 1.0661991 ]\n",
      "norm1-1/beta:0 [-0.0318516  -0.03053547 -0.02019857 -0.02790633 -0.02417356 -0.05364837\n",
      " -0.05368999 -0.03693485 -0.14365003 -0.03451983 -0.01717715  0.01241601\n",
      " -0.12787396 -0.05418214 -0.09771056 -0.06169211 -0.09586416 -0.00668785\n",
      " -0.05782411 -0.0577069  -0.02023308 -0.13516262 -0.0101125  -0.08427543\n",
      " -0.08018117 -0.07024402  0.01068826 -0.05463503 -0.09560614  0.0104282\n",
      " -0.14408645 -0.07832127 -0.10839453 -0.13842814 -0.08805194 -0.03459639\n",
      " -0.22935031 -0.02621482 -0.01142127 -0.0287776  -0.11017717 -0.04351072\n",
      " -0.05076755 -0.01753632 -0.04164392 -0.09380882 -0.10333394 -0.10452073\n",
      " -0.14937283 -0.01288579 -0.14558809 -0.02676197 -0.04043768 -0.06875548\n",
      " -0.06357945 -0.09683074 -0.07843614 -0.01714732 -0.03869227 -0.13904686\n",
      " -0.01298034 -0.09867838 -0.04976567 -0.02455038 -0.05808749 -0.18123849\n",
      " -0.03599993 -0.06136962 -0.0904429  -0.05429767 -0.07417169 -0.01184836\n",
      " -0.07495406 -0.08143187 -0.17526858 -0.15141316 -0.1791517  -0.03015155\n",
      " -0.07818446 -0.0517897  -0.00223697 -0.05859762 -0.04239626 -0.03706349\n",
      " -0.07095827 -0.05899139 -0.0441393  -0.04943812 -0.04607161 -0.03384136\n",
      " -0.11979021 -0.00548627 -0.00514719 -0.06409958 -0.04104619 -0.0595368 ]\n",
      "Conv_1/weights:0 [[[[-0.04332381 -0.02241262 -0.16302297 ...  0.00048841  0.14624149\n",
      "     0.03601088]\n",
      "   [-0.05321801 -0.05735189 -0.11914378 ... -0.06798215  0.10225274\n",
      "    -0.15672214]\n",
      "   [ 0.21613833 -0.09300895  0.04620708 ... -0.05282219 -0.0505449\n",
      "    -0.07371729]\n",
      "   ...\n",
      "   [-0.02776898 -0.07197941 -0.28619972 ... -0.12299095  0.0046848\n",
      "     0.07187513]\n",
      "   [ 0.07797787 -0.03119743 -0.12470754 ... -0.00438261 -0.21541181\n",
      "    -0.05592056]\n",
      "   [ 0.0259831  -0.06824077 -0.14651743 ... -0.05034083  0.11185464\n",
      "    -0.04263721]]\n",
      "\n",
      "  [[ 0.0404954  -0.13294916 -0.08631275 ... -0.0068454   0.05458352\n",
      "     0.06503197]\n",
      "   [-0.06050195 -0.05467675  0.0018826  ... -0.04383403  0.10318521\n",
      "    -0.2456492 ]\n",
      "   [ 0.19379209 -0.04191323  0.01680988 ...  0.06094569 -0.14718091\n",
      "     0.05827491]\n",
      "   ...\n",
      "   [-0.04478256 -0.10641801 -0.03948142 ... -0.23568836  0.16629888\n",
      "    -0.3433786 ]\n",
      "   [ 0.06059462 -0.04491818  0.19083337 ... -0.05757823  0.03642391\n",
      "    -0.1876705 ]\n",
      "   [ 0.05421005 -0.07115138 -0.10577309 ... -0.12954882  0.11556557\n",
      "    -0.06942423]]\n",
      "\n",
      "  [[ 0.08046116 -0.04902696 -0.05191672 ...  0.03478436 -0.17280594\n",
      "    -0.1328911 ]\n",
      "   [-0.0311774  -0.08574331 -0.00840122 ...  0.03208023 -0.02201279\n",
      "    -0.1036129 ]\n",
      "   [ 0.11201358 -0.07251997  0.08408245 ...  0.06400604 -0.14066653\n",
      "     0.00190059]\n",
      "   ...\n",
      "   [-0.09954473 -0.00994826  0.00495426 ... -0.17027648 -0.13554397\n",
      "     0.06185514]\n",
      "   [ 0.04676571 -0.02982855  0.03937604 ... -0.10229577 -0.01810852\n",
      "    -0.02215201]\n",
      "   [-0.01553172 -0.03481766  0.09163459 ... -0.0754903  -0.00170684\n",
      "    -0.17582706]]]\n",
      "\n",
      "\n",
      " [[[ 0.07688709 -0.1145959  -0.05147367 ... -0.03867489 -0.01121586\n",
      "     0.08109604]\n",
      "   [ 0.00429414 -0.06424644 -0.09635747 ... -0.08047314  0.13098074\n",
      "    -0.06790211]\n",
      "   [-0.01097586 -0.00983051 -0.02455295 ... -0.00086464 -0.10543416\n",
      "     0.03899133]\n",
      "   ...\n",
      "   [-0.02436383 -0.08649673 -0.1338     ... -0.14707002  0.01507849\n",
      "    -0.04970396]\n",
      "   [ 0.02604275  0.02219224  0.02864202 ...  0.00039456 -0.03786551\n",
      "    -0.05959276]\n",
      "   [ 0.05304894  0.00799691 -0.17133011 ... -0.19211735  0.09719929\n",
      "     0.00630226]]\n",
      "\n",
      "  [[-0.00974126 -0.08602884 -0.09959223 ... -0.05653583  0.02854124\n",
      "    -0.03105867]\n",
      "   [-0.0119704  -0.01153874 -0.06333812 ... -0.00937773  0.06933544\n",
      "     0.03490369]\n",
      "   [ 0.16749457 -0.11710247 -0.00913501 ...  0.04163614 -0.15004477\n",
      "    -0.07740849]\n",
      "   ...\n",
      "   [-0.1408116  -0.02995705 -0.2350917  ... -0.1466649   0.10447549\n",
      "    -0.22680824]\n",
      "   [-0.03603031 -0.04728454 -0.04579881 ... -0.00537744  0.07065191\n",
      "    -0.11778851]\n",
      "   [ 0.10921031 -0.01462553 -0.3099785  ... -0.3152769   0.07833458\n",
      "     0.00649767]]\n",
      "\n",
      "  [[ 0.03649516 -0.03704359 -0.0738758  ...  0.03771359 -0.2360647\n",
      "    -0.09971798]\n",
      "   [ 0.04145223 -0.03911299  0.09181003 ... -0.00684618 -0.1320104\n",
      "     0.00615962]\n",
      "   [ 0.07452391 -0.09426548  0.03080648 ...  0.08321369 -0.14157051\n",
      "    -0.09163403]\n",
      "   ...\n",
      "   [-0.08606618 -0.0043029   0.02330752 ... -0.2540897  -0.12118203\n",
      "     0.03127841]\n",
      "   [-0.07753897 -0.00259183  0.02953594 ... -0.13892722 -0.00499683\n",
      "     0.05541014]\n",
      "   [ 0.1187958  -0.04424204 -0.08750763 ... -0.16713679 -0.10046999\n",
      "    -0.10857899]]]\n",
      "\n",
      "\n",
      " [[[ 0.09898894 -0.03494607  0.18214937 ...  0.0432034  -0.0440252\n",
      "    -0.02663699]\n",
      "   [ 0.08211559 -0.0531226   0.07629303 ... -0.07408712  0.10634121\n",
      "     0.00647545]\n",
      "   [-0.08338149 -0.03877714 -0.07313438 ...  0.00449455 -0.16831788\n",
      "    -0.07622834]\n",
      "   ...\n",
      "   [-0.01348681 -0.05231939 -0.05690022 ... -0.07330348 -0.00162557\n",
      "    -0.1380397 ]\n",
      "   [-0.02120515 -0.00803879 -0.1033226  ... -0.08004595  0.01682939\n",
      "    -0.04439732]\n",
      "   [-0.06936572 -0.07506052  0.03811499 ... -0.10964841 -0.11509681\n",
      "    -0.0533534 ]]\n",
      "\n",
      "  [[ 0.1322232  -0.09613286  0.00178721 ... -0.02513079 -0.07338677\n",
      "    -0.07702073]\n",
      "   [ 0.04063199 -0.03261613 -0.0039566  ... -0.09027474 -0.02645758\n",
      "    -0.0007147 ]\n",
      "   [-0.08732058 -0.07229745 -0.04490605 ...  0.11852624 -0.14052776\n",
      "    -0.09678148]\n",
      "   ...\n",
      "   [-0.12296184 -0.00291124 -0.20684266 ... -0.22148947  0.11947239\n",
      "    -0.10720515]\n",
      "   [-0.07689577  0.04272847 -0.11032603 ... -0.09123029  0.14117484\n",
      "    -0.11466923]\n",
      "   [-0.07422503 -0.04222244 -0.05236479 ... -0.07855397 -0.0726684\n",
      "    -0.01886661]]\n",
      "\n",
      "  [[ 0.1028107  -0.13164231 -0.02261564 ...  0.01565084 -0.21061282\n",
      "    -0.03719146]\n",
      "   [ 0.08503522 -0.04504266  0.04968089 ... -0.01983605 -0.11998899\n",
      "     0.13218811]\n",
      "   [-0.09248802 -0.12008201  0.01537307 ...  0.03264339 -0.1449023\n",
      "    -0.05921581]\n",
      "   ...\n",
      "   [-0.04520221 -0.06727804 -0.14757332 ... -0.13889806 -0.13741308\n",
      "    -0.04967364]\n",
      "   [-0.0828045  -0.03901337  0.01749405 ... -0.11758909  0.00564377\n",
      "     0.03615128]\n",
      "   [ 0.05027179 -0.04803937 -0.25772592 ... -0.14605077 -0.06000278\n",
      "     0.08072768]]]]\n",
      "Conv_1/biases:0 [-0.14116398 -0.10391814 -0.14533767 -0.08821571 -0.06597027 -0.09070736\n",
      " -0.15679608 -0.08872256 -0.09678698 -0.1206484  -0.11488838 -0.09021758\n",
      " -0.05097447 -0.1951474  -0.09641888 -0.09098493 -0.01720155  0.04749294\n",
      " -0.0218759  -0.19748764 -0.08115257 -0.04377009 -0.03641659 -0.05826175\n",
      " -0.11251765 -0.01789505 -0.06228893  0.01931012 -0.05193525 -0.04384501\n",
      " -0.1295288  -0.02787117 -0.01343785 -0.07915219 -0.00451946 -0.06039183\n",
      "  0.01914663 -0.17207876  0.0096305   0.00065567 -0.0797427  -0.16827545\n",
      " -0.00556379  0.07004364 -0.02193545 -0.08465021 -0.16438252  0.00465326\n",
      " -0.11382773 -0.11822841  0.02600382  0.04168995 -0.11414731 -0.03942825\n",
      "  0.0195352  -0.07545841 -0.02144285 -0.138385   -0.02797573 -0.10990742\n",
      " -0.05744543 -0.02483441 -0.09461183  0.00079296 -0.14057638 -0.14171183\n",
      " -0.12210378 -0.01682735  0.00486206 -0.14853509 -0.08189175 -0.04782843\n",
      "  0.02942845 -0.08511686 -0.01416443 -0.02060304 -0.05277358 -0.03137243\n",
      " -0.1409284  -0.06902353 -0.1706299   0.01673879 -0.03383055 -0.15095374\n",
      " -0.01132031  0.0118999  -0.00282492 -0.11201563 -0.04608877 -0.1376476\n",
      " -0.06737994 -0.04390185 -0.11308581  0.00322791  0.04215417 -0.02626978]\n",
      "norm1-2/gamma:0 [0.9597975  0.946502   1.1976417  1.0714453  1.1153847  1.1000613\n",
      " 0.95013994 1.029421   0.9104479  1.0567456  0.94047296 0.8955707\n",
      " 1.077331   0.99215937 1.0351738  1.0488214  1.1850544  1.1089036\n",
      " 1.0795878  0.9823502  1.1307291  1.0532323  0.9995824  1.1571751\n",
      " 0.9048242  1.0412445  1.1123241  1.0908335  0.98587507 1.0243508\n",
      " 0.9791855  1.0542816  1.0859673  1.1692054  1.0125757  0.9626107\n",
      " 1.0449171  0.9395053  1.0399024  1.167072   1.2245886  1.0032116\n",
      " 0.9557199  1.0613364  1.0319273  0.95903856 1.089561   1.0915245\n",
      " 1.0781206  0.8329839  1.1150174  0.9211376  1.1122673  1.0971869\n",
      " 0.90447015 1.1851864  1.1240907  1.1047813  1.0386758  0.95797354\n",
      " 1.0161055  0.978599   0.9690275  1.1234906  1.0304065  1.1218158\n",
      " 0.8938273  1.1382326  1.0485574  0.9425658  1.1370655  1.0410261\n",
      " 1.0995592  0.9311447  1.1273973  0.91000575 0.94045454 1.0106598\n",
      " 1.131616   0.93391085 0.80493623 1.1000997  0.91651785 1.162657\n",
      " 1.0746778  1.1262627  1.2330506  1.0252751  1.1380394  0.98994935\n",
      " 0.9162198  0.95866925 1.1834052  1.1033535  1.3826551  1.0580387 ]\n",
      "norm1-2/beta:0 [-8.55736360e-02 -9.50879534e-04 -1.21941701e-01 -1.26772314e-01\n",
      " -1.42079517e-01 -2.03725845e-01 -1.68281659e-01 -1.05202816e-01\n",
      " -1.25118233e-02 -1.44368067e-01 -5.44832693e-03  2.27707205e-03\n",
      " -1.31979331e-01 -1.48558483e-01 -2.31104076e-01 -5.73659278e-02\n",
      " -6.28093779e-02 -1.02095805e-01 -1.26945704e-01 -1.13449551e-01\n",
      " -5.14432639e-02 -1.18537784e-01 -1.81384742e-01 -6.05118573e-02\n",
      " -1.29966959e-01 -1.84469342e-01 -1.53833568e-01 -1.01978593e-01\n",
      " -4.63615507e-02 -7.39884526e-02 -8.03695843e-02 -8.79430994e-02\n",
      " -1.28959522e-01 -1.97956458e-01 -7.77018964e-02 -1.72492534e-01\n",
      " -1.66046351e-01 -1.34981632e-01 -6.94988966e-02 -2.46931970e-01\n",
      " -1.14668012e-01 -1.15724005e-01 -4.78653098e-03 -9.05549154e-02\n",
      " -5.25385551e-02 -1.62150756e-01 -2.05074787e-01 -2.04103172e-01\n",
      " -1.10660791e-01 -1.51344622e-02 -1.22906737e-01 -2.26119868e-02\n",
      " -1.10146314e-01 -1.71546564e-01 -4.43236642e-02 -1.84051275e-01\n",
      " -6.37428835e-02 -1.68615520e-01 -8.05485900e-03 -1.23861894e-01\n",
      " -1.25639681e-02 -4.22999896e-02 -1.30734339e-01 -1.04781263e-01\n",
      " -1.05522744e-01 -6.45291433e-02  4.90924437e-03 -1.45040572e-01\n",
      " -1.21246912e-01 -1.67877972e-01 -1.01789795e-01 -1.02431290e-01\n",
      " -1.59070998e-01  7.13097677e-03 -1.45391732e-01 -9.10317525e-03\n",
      " -9.02080629e-03 -7.89689422e-02 -1.07607663e-01 -1.74349640e-04\n",
      " -5.75736724e-02 -1.61958471e-01 -1.60893966e-02 -9.42513943e-02\n",
      " -5.07587940e-02 -7.12487847e-02 -3.82411778e-02 -1.84496000e-01\n",
      " -1.28010899e-01 -2.09820457e-02  1.00455876e-03  1.47824707e-02\n",
      " -5.63781299e-02 -1.80378810e-01 -1.14397086e-01 -2.13731453e-01]\n",
      "Conv_2/weights:0 [[[[-4.64162827e-02  3.56333628e-02  7.86640197e-02 ... -1.90691248e-01\n",
      "    -7.52774924e-02 -2.55695730e-01]\n",
      "   [ 4.05336060e-02 -2.45526433e-02  4.74257991e-02 ... -8.21574032e-02\n",
      "     2.32040267e-02  4.37243171e-02]\n",
      "   [-1.26975968e-01  6.71509840e-03 -9.66005120e-03 ...  7.75801241e-02\n",
      "     4.99312952e-02  4.75958223e-03]\n",
      "   ...\n",
      "   [-9.54116359e-02  6.23647124e-02  7.19778612e-03 ...  1.24609224e-01\n",
      "    -1.20813407e-01 -3.74463797e-02]\n",
      "   [ 1.23596795e-01  1.85120199e-02  3.77596840e-02 ... -1.36495471e-01\n",
      "     1.14478625e-01 -1.14363886e-01]\n",
      "   [ 5.26335649e-02  3.14950086e-02 -1.22522628e-02 ...  6.98888004e-02\n",
      "     5.43892123e-02 -5.41483471e-03]]\n",
      "\n",
      "  [[-1.38632134e-01 -7.55957281e-03  7.56995240e-03 ... -3.12906086e-01\n",
      "    -7.14522004e-02 -2.55392760e-01]\n",
      "   [ 4.72466834e-02  1.27783446e-02 -3.76374647e-02 ... -6.45229667e-02\n",
      "     1.37544172e-02  3.45114172e-02]\n",
      "   [-4.43248957e-01  7.03952834e-02  2.15283826e-01 ... -1.32637635e-01\n",
      "     6.81781843e-02 -2.42161304e-01]\n",
      "   ...\n",
      "   [-8.73727575e-02  2.53128946e-01  1.93648878e-02 ...  1.54454321e-01\n",
      "    -4.24144790e-02 -4.82015349e-02]\n",
      "   [-1.42029181e-01 -3.72568867e-03 -6.67205527e-02 ...  6.22858573e-03\n",
      "    -4.60861810e-02 -1.22752272e-01]\n",
      "   [ 3.17660943e-02 -8.13200995e-02 -4.36920747e-02 ... -5.62695973e-03\n",
      "     1.00357607e-01  6.12370186e-02]]\n",
      "\n",
      "  [[-4.65347320e-02  9.10757184e-02 -1.26166607e-03 ... -7.85803944e-02\n",
      "    -2.14309013e-03 -2.62558132e-01]\n",
      "   [-4.11435887e-02  4.85815033e-02  3.82309556e-02 ... -4.50811051e-02\n",
      "     1.58491004e-02  2.78448593e-02]\n",
      "   [-3.03027868e-01  1.24623708e-01  4.93588820e-02 ... -1.07376829e-01\n",
      "    -8.30433914e-04 -1.68783054e-01]\n",
      "   ...\n",
      "   [-5.04997075e-02  1.03963509e-01 -1.06176063e-01 ...  1.69245824e-01\n",
      "     4.03835555e-04 -2.22605579e-02]\n",
      "   [-7.93485641e-02 -3.42485979e-02 -4.23299037e-02 ...  9.49750841e-02\n",
      "    -1.38768271e-01 -3.08880568e-01]\n",
      "   [ 1.36753172e-01  1.51354536e-01 -7.03695491e-02 ... -7.28190411e-03\n",
      "     7.09034130e-02  9.73688588e-02]]]\n",
      "\n",
      "\n",
      " [[[ 1.24646164e-02 -3.39556150e-02 -1.16048623e-02 ... -9.67472568e-02\n",
      "    -5.90149090e-02 -4.64495085e-02]\n",
      "   [ 1.94434095e-02  4.97580580e-02  1.89477950e-02 ... -1.43878981e-02\n",
      "    -3.62212844e-02 -1.79584939e-02]\n",
      "   [ 2.13903755e-01  3.37943994e-02  1.19748294e-01 ...  5.67936301e-02\n",
      "    -1.00202095e-02  1.93601087e-01]\n",
      "   ...\n",
      "   [-8.15609545e-02 -3.44667360e-02  1.52736679e-01 ...  1.29760399e-01\n",
      "    -2.06937697e-02 -1.10517755e-01]\n",
      "   [ 4.17938121e-02  1.05837658e-01  1.35216072e-01 ...  2.90822219e-02\n",
      "    -1.29392579e-01 -1.16862409e-01]\n",
      "   [ 1.22806847e-01  5.23713902e-02 -1.38082191e-01 ...  7.91401789e-02\n",
      "     1.31121948e-01 -1.06788106e-01]]\n",
      "\n",
      "  [[-1.06909491e-01  3.29916030e-02  7.73387998e-02 ... -3.05479646e-01\n",
      "     5.24647161e-02 -3.60484570e-02]\n",
      "   [-2.36146785e-02  1.11293411e-02  4.43371832e-02 ... -1.55923609e-02\n",
      "    -1.26509489e-02 -6.28406322e-03]\n",
      "   [-2.92246398e-02  9.60891247e-02  8.05909708e-02 ... -5.17758429e-02\n",
      "    -8.34327564e-02  4.60817218e-02]\n",
      "   ...\n",
      "   [-1.59651563e-01  7.48574659e-02  1.54363245e-01 ...  9.10369083e-02\n",
      "    -6.86535984e-02 -2.33508795e-02]\n",
      "   [-3.12763393e-01  1.07492417e-01 -7.15376362e-02 ...  3.33449757e-03\n",
      "    -3.75835150e-01 -1.35645688e-01]\n",
      "   [ 6.31685629e-02  1.00308945e-02  6.65721074e-02 ...  7.94699490e-02\n",
      "     8.72109830e-02  5.44778034e-02]]\n",
      "\n",
      "  [[-7.26881400e-02  9.02779326e-02  2.90521961e-02 ... -1.47064045e-01\n",
      "    -7.15574548e-02 -3.83215360e-02]\n",
      "   [-4.54531945e-02  6.38347268e-02 -1.87806673e-02 ... -6.22166414e-03\n",
      "    -3.33137698e-02 -7.04176649e-02]\n",
      "   [-3.22940797e-01  9.46300179e-02  4.25636694e-02 ... -1.14101268e-01\n",
      "    -6.18788078e-02 -2.42305733e-02]\n",
      "   ...\n",
      "   [-1.40557662e-01  1.36825740e-01 -5.77968918e-02 ...  1.48679197e-01\n",
      "     1.95594169e-02 -5.49022555e-02]\n",
      "   [-1.48813650e-01  8.95521492e-02 -4.90890890e-02 ...  2.72359792e-02\n",
      "    -2.27548808e-01 -1.81850448e-01]\n",
      "   [ 1.06124006e-01  2.68418521e-01  4.69715334e-02 ...  2.28046943e-02\n",
      "     1.80308744e-02  8.30889121e-02]]]\n",
      "\n",
      "\n",
      " [[[-3.96671109e-02 -4.17518243e-02 -5.45710959e-02 ...  2.34687515e-03\n",
      "    -1.68898760e-03  1.92817062e-01]\n",
      "   [ 2.66074315e-02 -1.70276978e-03 -6.07446246e-02 ...  4.37281467e-02\n",
      "     5.41063398e-03 -4.53288518e-02]\n",
      "   [ 3.10576200e-01 -2.99890339e-01  9.48798433e-02 ... -6.00815900e-02\n",
      "     5.06441295e-02  2.09524482e-01]\n",
      "   ...\n",
      "   [ 1.26422688e-01 -8.29743445e-02  5.70516586e-02 ...  6.86398968e-02\n",
      "    -4.72175553e-02 -5.80240414e-02]\n",
      "   [ 9.54482611e-03  9.90061983e-02  1.28864989e-01 ...  3.06938961e-02\n",
      "    -1.25297874e-01  1.52853146e-01]\n",
      "   [ 1.12321012e-01 -2.71195229e-02 -3.22351843e-01 ...  9.94213596e-02\n",
      "    -4.01424803e-03 -7.15450644e-02]]\n",
      "\n",
      "  [[-3.30191180e-02 -2.90023852e-02 -2.14257184e-02 ... -9.57784578e-02\n",
      "    -3.26615223e-03  1.98708996e-02]\n",
      "   [ 2.22335514e-02  4.37910110e-03 -6.37572855e-02 ...  4.58314782e-03\n",
      "    -3.02637499e-02 -2.18736846e-02]\n",
      "   [ 5.44700364e-04  2.13391762e-02 -1.52275190e-01 ... -4.96769249e-02\n",
      "    -2.10799333e-02  1.52450010e-01]\n",
      "   ...\n",
      "   [-6.14944398e-02  5.32152168e-02 -1.56545676e-02 ...  6.69786558e-02\n",
      "    -2.55681109e-02 -1.22929014e-01]\n",
      "   [-1.01944380e-01  1.14112630e-01  4.88724709e-02 ...  2.57151984e-02\n",
      "    -4.34757918e-01 -1.32161975e-01]\n",
      "   [ 8.98426622e-02 -7.18296841e-02  1.47172868e-01 ...  1.57070056e-01\n",
      "    -5.42507581e-02  6.66235387e-02]]\n",
      "\n",
      "  [[-7.84818158e-02  1.69884395e-02 -1.90504473e-02 ... -6.79877996e-02\n",
      "    -9.48965997e-02  1.20934416e-02]\n",
      "   [-6.22844743e-03 -4.55119684e-02  8.46345909e-03 ... -3.33922617e-02\n",
      "    -5.37042767e-02  2.58693867e-03]\n",
      "   [-2.24498197e-01  7.86600709e-02  6.86673587e-03 ... -1.27703503e-01\n",
      "     7.91877955e-02 -1.63834229e-01]\n",
      "   ...\n",
      "   [-1.08061470e-01  1.57851670e-02  2.59773061e-02 ...  8.99995565e-02\n",
      "     2.08456200e-02 -9.60983932e-02]\n",
      "   [-2.12229658e-02  2.17207640e-01  1.18188970e-01 ...  1.05921797e-01\n",
      "    -3.72067928e-01 -1.17640063e-01]\n",
      "   [ 1.96266659e-02  3.07253320e-02  1.37909725e-01 ...  1.24790236e-01\n",
      "    -1.19774356e-01  3.56115513e-02]]]]\n",
      "Conv_2/biases:0 [-0.03937776 -0.046053   -0.15358454 -0.04153723 -0.04225092  0.00738381\n",
      "  0.01559731  0.06588457 -0.06511157 -0.05551521 -0.05485573 -0.00225971\n",
      " -0.1228836  -0.05687845  0.00202996  0.00275259 -0.0458307  -0.05212935\n",
      "  0.00243084  0.06956526 -0.07048632  0.02503948 -0.10228122 -0.09344483\n",
      " -0.11177441 -0.03658246 -0.13259697 -0.09445861  0.05434807 -0.09601298\n",
      "  0.0798156  -0.06445458  0.01082734  0.0231283   0.08414662 -0.02148215\n",
      " -0.0752251   0.05874138 -0.05790021 -0.04499678 -0.0385487   0.05284672\n",
      "  0.04671415  0.03977076  0.051805   -0.01642785  0.04338706 -0.04773803\n",
      " -0.0515772  -0.0406259   0.00438398  0.03208926 -0.09848497 -0.10632986\n",
      " -0.05581606  0.02608877  0.11470462 -0.00137663  0.06304737 -0.05010328\n",
      " -0.1816435   0.04104705 -0.04208231 -0.13119124 -0.03935604  0.03608718\n",
      " -0.01497373 -0.07567715 -0.03340012 -0.12197877  0.07610288 -0.00713898\n",
      "  0.06313058  0.035719   -0.08139457 -0.02818224  0.03950507 -0.11857573\n",
      " -0.02462124 -0.1339427  -0.01369642 -0.00963249 -0.02171854  0.03048182\n",
      "  0.0486748   0.07759956 -0.03488743 -0.06671919 -0.10660227 -0.07576599\n",
      "  0.00241409  0.09592687  0.01820476 -0.03903979  0.04779627  0.00430224\n",
      " -0.01566593  0.02842561  0.06991247 -0.04441191  0.10710644  0.06707064\n",
      "  0.04001859 -0.09742087 -0.06362578  0.08397119 -0.09057035 -0.05248997\n",
      "  0.06416217 -0.00790041 -0.03036377  0.1052224   0.14329432  0.02295625\n",
      "  0.05349949 -0.01175989  0.10373563 -0.03876138  0.06685866 -0.01945055\n",
      "  0.00838952 -0.06979804 -0.12538841 -0.0540329  -0.00564765 -0.04320825\n",
      " -0.00694575 -0.12435944 -0.00110813 -0.00425625 -0.08203229 -0.02295271\n",
      "  0.02889994 -0.05821267 -0.00749681 -0.09484748 -0.01278041 -0.13786064\n",
      " -0.05418269 -0.03091525 -0.0728184   0.03102587 -0.01351971 -0.014506\n",
      "  0.06702214 -0.07275145 -0.02113821 -0.06295475 -0.05851496  0.03732498\n",
      "  0.06836119 -0.01720137 -0.00390284  0.02993949 -0.02161575 -0.00318946\n",
      " -0.03618383 -0.00926938 -0.00691208  0.08759978  0.02786098 -0.0830181\n",
      " -0.02161383  0.11443426 -0.04121177  0.04794961 -0.01219193 -0.01208648\n",
      " -0.07833125 -0.03382587 -0.05625742 -0.08330673  0.05539875  0.08071636\n",
      "  0.09853703 -0.06590563  0.07908204 -0.10669906 -0.01646402 -0.12234098\n",
      "  0.08653489 -0.01792343  0.09771612 -0.04917546 -0.02424455 -0.11430927\n",
      "  0.04639727 -0.0287139   0.05578655  0.04353311 -0.0344701  -0.14657035]\n",
      "norm2-1/gamma:0 [1.2094206  0.993594   1.0905491  1.2113664  1.0496839  1.1159263\n",
      " 1.0582558  1.1560756  1.0875567  1.1362906  1.1205375  1.0264171\n",
      " 1.1107328  1.1118357  1.2396679  1.2299685  0.9884559  1.1177361\n",
      " 1.0511817  1.2804486  1.003482   1.1046848  0.87321943 0.99241644\n",
      " 0.9324691  1.2874049  0.7565619  1.1154038  1.1961628  1.1388412\n",
      " 1.210388   1.0015755  1.1591976  1.1631697  1.1542978  1.1821661\n",
      " 1.1434774  1.0473984  1.2178836  1.0042028  1.090714   1.1794423\n",
      " 1.0995381  1.1265662  1.015436   1.2594922  1.0521961  0.94337845\n",
      " 1.053952   1.1140537  1.0296717  1.1459376  0.9012519  1.0679953\n",
      " 1.1961297  1.1196563  1.1931857  1.1158067  1.1224047  1.1013602\n",
      " 1.1046154  1.221888   1.1047355  0.94362605 1.284565   1.1722382\n",
      " 1.0507752  1.0629494  0.9673784  0.8926671  1.205139   1.1179763\n",
      " 1.0638162  1.189507   0.97262913 1.2171886  1.1302671  1.1963649\n",
      " 1.017409   0.8091684  0.8964166  1.1329043  1.2881285  1.210937\n",
      " 1.2005335  1.2406969  1.0590998  1.0097533  0.9288556  1.216331\n",
      " 1.3086668  1.1406387  1.1929177  1.1609629  1.2427633  1.0409144\n",
      " 1.2403272  1.168889   1.2496837  1.1229776  1.2352465  1.2083871\n",
      " 1.1407912  1.1583378  0.9445159  1.1833261  1.1825563  1.1605829\n",
      " 1.1588926  1.1889352  1.1025412  1.0690796  1.1773903  1.2108504\n",
      " 1.2344514  0.93662745 1.3242149  1.2430295  1.1594269  1.0553753\n",
      " 1.0301806  1.0015755  0.9195785  1.1646645  0.97289675 1.1127456\n",
      " 1.0445898  1.0030724  1.1802896  1.0992336  1.0377258  1.0718036\n",
      " 1.1208214  1.1687139  1.1069213  0.9669725  0.98532677 1.0162097\n",
      " 1.1577429  0.96645653 1.0859928  1.1503576  1.1060704  1.140891\n",
      " 1.1439859  1.0404814  1.187489   1.0914605  1.0338743  1.2072229\n",
      " 1.1956208  1.1729925  1.0667591  1.0792695  1.0160788  1.1008941\n",
      " 0.9778375  1.0875119  1.1420958  1.1787378  1.1471152  1.1014026\n",
      " 1.1579418  1.1973039  1.0112638  1.0488352  1.1141413  1.0143255\n",
      " 0.8964341  1.1369419  1.1060553  1.2889221  1.1627697  1.2859504\n",
      " 1.1433256  1.0482358  1.1877676  1.1126672  1.1318631  1.0760008\n",
      " 1.0738633  1.085187   1.2266216  0.9538558  1.1429266  1.1752629\n",
      " 1.133189   1.0594361  1.1919981  1.147311   0.9604188  1.266217  ]\n",
      "norm2-1/beta:0 [-0.13759373 -0.08751285 -0.17545742 -0.17059977 -0.08838121 -0.05987794\n",
      " -0.04652353 -0.01036279 -0.16345088 -0.11710767 -0.11955609 -0.06983447\n",
      " -0.13918947 -0.1025317  -0.11388184 -0.03805908 -0.11452453 -0.11097591\n",
      " -0.09714779 -0.06960323 -0.10745681 -0.10709463 -0.09756172 -0.13825633\n",
      " -0.16006148 -0.06135545 -0.0498835  -0.07856202 -0.0398121  -0.09647795\n",
      " -0.03516117 -0.10341212 -0.03202823 -0.06518087 -0.08384645 -0.09626362\n",
      " -0.13980626 -0.04909424 -0.06794863 -0.10001283 -0.07046034 -0.09178948\n",
      " -0.07989687 -0.10405022 -0.13016732 -0.06977943 -0.11439863 -0.08589567\n",
      " -0.1424263  -0.12686108 -0.1409486  -0.10486931 -0.1460251  -0.18507761\n",
      " -0.12027701 -0.05795323 -0.11545458 -0.13373804 -0.0874822  -0.17978817\n",
      " -0.18054995 -0.060955   -0.06141125 -0.17563744 -0.04816466  0.00106714\n",
      " -0.18263985 -0.13744298 -0.10264866 -0.08967143 -0.03017237 -0.07815184\n",
      " -0.1216903  -0.05050912 -0.10849032 -0.07775038 -0.03315476 -0.1404827\n",
      " -0.06866916 -0.1548656  -0.07885614 -0.10521362 -0.04091106 -0.0481311\n",
      " -0.11651765 -0.08329668 -0.08827333 -0.10207918 -0.1251774  -0.10879403\n",
      " -0.02486282 -0.00579028 -0.05273125 -0.04857992 -0.11175138 -0.06778979\n",
      " -0.0195615  -0.0431872  -0.10494806 -0.13064176 -0.0703193  -0.11581418\n",
      " -0.03018507 -0.2189193  -0.14439163 -0.03583333 -0.08138822 -0.09024702\n",
      " -0.05232742 -0.09319362 -0.03236469 -0.09298443 -0.0551312  -0.05963067\n",
      " -0.06613759 -0.06973657 -0.0952936  -0.13575761 -0.11476254 -0.08508754\n",
      " -0.1509838  -0.17859979 -0.1496808  -0.12010342 -0.10938069 -0.13241394\n",
      " -0.08280908 -0.11958089 -0.13100158 -0.1508212  -0.14028396 -0.1861339\n",
      " -0.10361652 -0.15223016 -0.12998323 -0.13656619 -0.08415233 -0.05096288\n",
      " -0.11259186 -0.12711258 -0.16476974 -0.05649558 -0.10971453 -0.05116608\n",
      " -0.0995523  -0.10908498 -0.12752369 -0.11077454 -0.08466115 -0.0737263\n",
      " -0.08759671 -0.07367738 -0.13210495 -0.03685031 -0.07073611 -0.08045256\n",
      " -0.13363832 -0.06613708 -0.08904284 -0.02461439 -0.1236212  -0.18190917\n",
      " -0.10484879 -0.04720051 -0.11236281 -0.07842828 -0.1606413  -0.12634067\n",
      " -0.12497235 -0.14677435 -0.08863021 -0.07565225 -0.08433516 -0.02088012\n",
      " -0.08716661 -0.15549853 -0.12314179 -0.234059   -0.06166587 -0.12001831\n",
      " -0.05519733 -0.079551   -0.01741029 -0.04418796 -0.07880568 -0.12465269\n",
      " -0.08988921 -0.06872237 -0.07277679 -0.00466534 -0.10054222 -0.08273506]\n",
      "Conv_3/weights:0 [[[[ 9.44892585e-04  1.10717997e-01 -1.19172670e-01 ... -1.23093650e-01\n",
      "    -1.14068091e-01 -2.03840807e-01]\n",
      "   [ 1.53339356e-02 -1.18505033e-02 -2.49367934e-02 ... -4.28447276e-02\n",
      "    -3.38447234e-03 -1.30118504e-01]\n",
      "   [ 4.46862727e-02 -8.11334327e-02 -2.25235685e-03 ...  2.32665256e-01\n",
      "    -1.61544561e-01 -4.16674241e-02]\n",
      "   ...\n",
      "   [-6.61897734e-02 -1.11096725e-01 -1.59607545e-01 ...  1.22473106e-01\n",
      "    -2.18191147e-02 -2.72193030e-02]\n",
      "   [-7.75491295e-04 -1.58834204e-01  6.16191439e-02 ...  6.45355284e-02\n",
      "    -5.30896224e-02 -4.83845100e-02]\n",
      "   [ 1.68853090e-03 -1.10163791e-02  6.00127988e-02 ...  2.66807705e-01\n",
      "    -2.19257455e-02  9.19474587e-02]]\n",
      "\n",
      "  [[-5.10495156e-02  3.77400243e-03 -1.71830729e-01 ... -5.72053753e-02\n",
      "     1.96459338e-01 -2.01656759e-01]\n",
      "   [ 5.72174415e-02  7.47515410e-02  5.77745086e-04 ... -3.07407565e-02\n",
      "    -4.21659723e-02  4.86494228e-02]\n",
      "   [-1.26400739e-01 -6.88373372e-02  1.40951499e-01 ...  1.16217747e-01\n",
      "     1.27751291e-01  3.21089812e-02]\n",
      "   ...\n",
      "   [ 6.67652860e-02  9.80744585e-02 -1.42965943e-01 ...  1.29773235e-02\n",
      "    -1.75110176e-01 -4.13436107e-02]\n",
      "   [ 8.18900540e-02 -8.77526626e-02 -3.19008455e-02 ... -3.89039256e-02\n",
      "    -2.41580736e-02 -1.06647499e-01]\n",
      "   [ 4.30069789e-02 -6.78171590e-02 -2.10280139e-02 ...  6.77111447e-02\n",
      "    -3.88945527e-02  1.36149198e-01]]\n",
      "\n",
      "  [[-5.93069270e-02 -1.52662471e-01 -1.29677713e-01 ...  7.41292238e-02\n",
      "     2.55666763e-01  6.21205419e-02]\n",
      "   [ 4.86213528e-02 -2.41564382e-02 -8.24445933e-02 ... -1.03391007e-01\n",
      "    -3.17478627e-01 -8.24298412e-02]\n",
      "   [-2.68149674e-02 -1.71226487e-01  2.44564742e-01 ...  1.51191145e-01\n",
      "    -3.17510031e-02  1.07546868e-02]\n",
      "   ...\n",
      "   [ 4.78733331e-02  5.06002232e-02 -6.02230094e-02 ...  7.01248199e-02\n",
      "     5.51310778e-02 -4.32687886e-02]\n",
      "   [ 2.33317595e-02  4.50444743e-02 -1.11339457e-01 ... -5.65084117e-03\n",
      "     7.35893920e-02 -2.54263043e-01]\n",
      "   [ 9.08538848e-02 -1.11476324e-01 -9.30281207e-02 ...  2.69826710e-01\n",
      "     1.25169230e-03 -1.01385221e-01]]]\n",
      "\n",
      "\n",
      " [[[ 3.22683379e-02 -4.85894047e-02  1.39063939e-01 ...  3.85781154e-02\n",
      "    -1.02099687e-01 -4.36575664e-03]\n",
      "   [ 8.24627876e-02 -1.61564305e-01  9.80753154e-02 ...  9.25663486e-02\n",
      "     3.14319842e-02 -1.04604550e-01]\n",
      "   [ 2.69367639e-02  5.61244078e-02  1.59337074e-02 ...  1.10614195e-01\n",
      "    -1.64051533e-01 -9.98207256e-02]\n",
      "   ...\n",
      "   [-5.02123013e-02  1.36457428e-01 -6.49741814e-02 ... -1.41834125e-01\n",
      "     2.87949224e-03 -1.78955663e-02]\n",
      "   [-1.18942223e-02 -9.78595763e-02 -5.43836094e-02 ...  1.42606646e-01\n",
      "    -1.22205935e-01  4.10918258e-02]\n",
      "   [-1.29664596e-02  9.66014937e-02 -5.98473810e-02 ...  3.81598175e-02\n",
      "     4.69159298e-02  1.45267352e-01]]\n",
      "\n",
      "  [[ 1.77590977e-02 -8.23813230e-02 -1.43467933e-01 ...  9.79278162e-02\n",
      "     3.45491618e-02 -9.92219597e-02]\n",
      "   [ 3.28556001e-02 -7.97426328e-02  8.74180719e-02 ...  5.65755405e-02\n",
      "    -4.01369408e-02 -2.38204394e-02]\n",
      "   [-1.65968701e-01  3.19564412e-03 -2.19434872e-02 ... -3.50960623e-03\n",
      "    -3.60918529e-02  2.33242032e-03]\n",
      "   ...\n",
      "   [ 8.43987763e-02  1.75287798e-01 -3.99046838e-02 ... -1.48951128e-01\n",
      "    -1.74208418e-01 -1.40026540e-01]\n",
      "   [ 5.81127964e-02 -2.25532144e-01 -8.02800804e-02 ...  1.30530715e-01\n",
      "     6.29984066e-02  7.10712448e-02]\n",
      "   [-1.06524624e-01  6.85116053e-02  8.61627087e-02 ...  8.15816298e-02\n",
      "     5.94127402e-02  6.63777515e-02]]\n",
      "\n",
      "  [[-2.46960577e-02 -5.59857115e-02  3.55615765e-02 ... -7.12902099e-02\n",
      "     2.36978099e-01  6.94842190e-02]\n",
      "   [ 1.56148095e-02 -9.44106802e-02  2.84203999e-02 ...  1.30953297e-01\n",
      "    -1.36534870e-01 -9.57510844e-02]\n",
      "   [-2.71869618e-02 -1.65224224e-01  5.49766701e-03 ...  8.48162323e-02\n",
      "     5.95829152e-02  1.48015134e-02]\n",
      "   ...\n",
      "   [ 1.64150465e-02  1.49672881e-01 -5.41931204e-02 ... -5.78793958e-02\n",
      "    -1.33985370e-01 -1.47550389e-01]\n",
      "   [ 9.99911651e-02 -1.51473999e-01 -3.13951612e-01 ...  8.33117366e-02\n",
      "     8.31820304e-04 -4.17309739e-02]\n",
      "   [-1.30257875e-01 -5.58559224e-02  9.37782228e-03 ...  9.21571702e-02\n",
      "    -1.09700859e-01  7.79090077e-02]]]\n",
      "\n",
      "\n",
      " [[[-9.98367816e-02 -1.27366126e-01  1.21922903e-01 ...  2.66694110e-02\n",
      "     6.20483384e-02  1.01913609e-01]\n",
      "   [-3.45150684e-03  1.19580291e-01  2.22177953e-02 ... -4.96171974e-02\n",
      "     1.89207178e-02 -3.90085578e-02]\n",
      "   [-1.36610627e-01 -1.03700645e-01  3.77329290e-02 ...  8.27650353e-02\n",
      "    -1.06206857e-01 -1.74215034e-01]\n",
      "   ...\n",
      "   [ 3.82413864e-02  6.79882765e-02 -2.84275711e-02 ... -1.92024916e-01\n",
      "     7.95533061e-02 -3.88999917e-02]\n",
      "   [ 7.43440688e-02 -2.95367949e-02 -2.69740038e-02 ... -1.48408085e-01\n",
      "    -1.00431591e-01 -3.93290780e-02]\n",
      "   [-3.87468077e-02  7.60909403e-03 -1.11578092e-01 ... -1.31917829e-02\n",
      "     1.17432311e-01  2.47917022e-04]]\n",
      "\n",
      "  [[ 1.23435713e-01 -2.91866183e-01  2.09026616e-02 ... -4.61796671e-02\n",
      "    -2.62453388e-02 -5.85989542e-02]\n",
      "   [ 7.29896035e-03 -1.30627090e-02  8.84192064e-02 ... -5.61960861e-02\n",
      "     1.14742825e-02  1.21946065e-02]\n",
      "   [-2.17549894e-02 -1.09183140e-01 -6.90728724e-02 ... -8.59709978e-02\n",
      "    -8.15453380e-02 -1.28090188e-01]\n",
      "   ...\n",
      "   [-1.94643438e-02  1.27814710e-01 -4.96347854e-03 ...  1.75671484e-02\n",
      "     8.53869244e-02 -6.99441209e-02]\n",
      "   [ 5.00637889e-02 -1.15619168e-01 -2.45919660e-01 ... -1.33546546e-01\n",
      "    -5.09406328e-02  3.11078429e-02]\n",
      "   [-9.50633660e-02  1.08375877e-01 -1.69562697e-01 ...  2.77426153e-01\n",
      "    -1.21354528e-01 -2.63515502e-01]]\n",
      "\n",
      "  [[ 1.21200643e-01 -7.72188753e-02 -1.65343300e-01 ... -4.87504639e-02\n",
      "     8.48001614e-02 -2.86541786e-02]\n",
      "   [ 1.51680866e-02 -1.72561593e-02  7.36701787e-02 ...  1.26388863e-01\n",
      "     2.08638310e-02 -1.20062776e-01]\n",
      "   [-3.84703791e-03 -2.76223533e-02  4.73548379e-03 ... -3.27867329e-01\n",
      "     9.33437701e-03 -7.92814791e-02]\n",
      "   ...\n",
      "   [-1.09520825e-02  1.72995314e-01 -1.28809102e-02 ... -9.25698653e-02\n",
      "     4.30628173e-02 -2.90793985e-01]\n",
      "   [-1.91859491e-02 -1.30364612e-01 -2.80550361e-01 ... -2.60065054e-03\n",
      "    -4.25359607e-02  3.12068798e-02]\n",
      "   [-8.31217766e-02  9.06021297e-02 -3.42276901e-01 ...  3.46823692e-01\n",
      "    -5.02717681e-02  1.35008007e-01]]]]\n",
      "Conv_3/biases:0 [ 0.02722852 -0.00089124 -0.01852239 -0.00403796  0.03777363 -0.03900617\n",
      "  0.00201666 -0.12191448 -0.04652991  0.10487588 -0.06286472 -0.06789824\n",
      " -0.07207818 -0.03462074  0.05383663 -0.01829326  0.02546929 -0.08405903\n",
      " -0.07967912 -0.02929505 -0.13266374 -0.0660143   0.00648672 -0.05787269\n",
      " -0.0776154  -0.02006916 -0.03940135  0.00085345 -0.0453141  -0.0257258\n",
      " -0.06645577 -0.00180923  0.09065732  0.05989208 -0.04400263  0.00956102\n",
      " -0.01164164  0.02396182  0.06181597 -0.0815631  -0.1334078  -0.03130904\n",
      " -0.04295801 -0.10501951 -0.0606712   0.00347281  0.08129658 -0.05352104\n",
      " -0.07463783 -0.0324339  -0.13844828 -0.04573845 -0.06742185  0.09890959\n",
      " -0.00822155 -0.11364138 -0.12210165 -0.03972193 -0.12172938 -0.08853333\n",
      " -0.00512074  0.09753859 -0.07911973 -0.12293407  0.01067389 -0.01186493\n",
      " -0.09002395 -0.08275435 -0.03130454 -0.07140888 -0.1480627  -0.10204808\n",
      "  0.04392209 -0.00679455  0.02467773 -0.03899297 -0.00211999 -0.10566176\n",
      " -0.0152263  -0.07246879 -0.06339836  0.05710469  0.04383444 -0.10523388\n",
      "  0.01517806 -0.04022463  0.01009384  0.03633329 -0.07434585  0.03806221\n",
      "  0.03049009 -0.0819333  -0.04071508  0.01351771  0.0030662   0.04725024\n",
      " -0.00433525 -0.00306668 -0.08615557 -0.01749391  0.00872779 -0.05544079\n",
      "  0.05294486  0.01031943 -0.07326312 -0.03061026 -0.0915906  -0.02400799\n",
      " -0.08488874 -0.03986503  0.08109296  0.04339189  0.00304994 -0.00088405\n",
      "  0.03554455  0.03633143 -0.10014218 -0.03322157 -0.0352257   0.01224563\n",
      "  0.04055789 -0.05163529 -0.01770592  0.0608171  -0.11634275 -0.00166714\n",
      " -0.11643261  0.03060232 -0.05161038 -0.12361773  0.03965055  0.02176966\n",
      "  0.04155355 -0.02965737  0.00516673 -0.02632778  0.00763818 -0.12015667\n",
      " -0.03363091 -0.06898371  0.00559841 -0.14706798  0.04181455 -0.07922148\n",
      "  0.09925085 -0.1390914  -0.10427091 -0.13420974 -0.00568602 -0.01318984\n",
      " -0.00545367 -0.00476183 -0.01423933 -0.07044458 -0.04334084 -0.03590144\n",
      "  0.05054627 -0.05312167  0.02884448 -0.08111673  0.10519387 -0.03078115\n",
      "  0.02318717 -0.16642527 -0.0372581  -0.0852127  -0.01538042 -0.01931292\n",
      " -0.08978811 -0.04541249 -0.06713243 -0.06050999 -0.07212338 -0.06740351\n",
      " -0.04194245  0.10016399  0.02114384  0.0002899  -0.10484214 -0.02748901\n",
      " -0.12056089  0.02099062  0.05155164 -0.07294688 -0.05123633 -0.06441775\n",
      " -0.04037789 -0.05637432 -0.02495139 -0.03645495 -0.04651164 -0.10168353]\n",
      "norm2-2/gamma:0 [1.0231829  1.1510996  1.1515863  1.1503494  1.2061992  1.0587517\n",
      " 1.1574993  0.9309719  1.0925077  1.211336   1.0274671  1.1363025\n",
      " 1.0672818  1.0703431  1.1500087  1.1306131  1.1528575  1.0935788\n",
      " 1.059928   1.0835018  0.8981144  1.0789132  1.0982187  1.064811\n",
      " 1.1003615  1.1424931  1.0882022  1.0870212  1.0496833  1.0769327\n",
      " 1.0754515  1.1115226  1.1474991  1.175024   1.1722064  1.0702622\n",
      " 1.1706288  1.1561335  1.1029977  1.0423515  1.0988705  1.1266183\n",
      " 0.9594514  1.0909991  1.0398672  1.123344   1.1586667  1.2653174\n",
      " 1.0484445  1.1125778  0.88959724 1.1664473  1.0902569  1.280013\n",
      " 1.0873306  1.1025133  0.9710066  1.1224728  0.8666756  1.0046059\n",
      " 1.1165043  1.2450739  1.1636279  1.0577028  1.0470597  1.1060288\n",
      " 1.1047682  1.1409498  1.1815928  1.0249007  1.081647   0.9903023\n",
      " 1.1544006  1.0951451  1.1400322  1.1207669  1.0515766  1.0518368\n",
      " 1.1158457  1.0033364  1.18155    1.2008514  1.0544355  1.0517272\n",
      " 1.0947615  1.0150107  1.0525862  1.0373464  1.0612394  1.0738524\n",
      " 1.104419   1.1502402  1.0866324  1.0457126  1.1789297  1.2245452\n",
      " 1.1060923  1.1566584  0.981613   1.0928862  1.1494632  1.1204902\n",
      " 1.1428553  1.0282773  1.0402433  1.1053689  1.08714    1.0900419\n",
      " 1.1134812  1.0752429  1.1631763  1.2094297  1.2003706  1.149822\n",
      " 1.1604402  1.1637272  1.1573819  1.0806149  0.99888307 1.112275\n",
      " 1.1628113  1.0890365  1.200619   1.2267294  0.82245    1.1686774\n",
      " 1.0535741  1.1355426  1.0138309  1.0882119  1.0627496  1.0247556\n",
      " 1.055976   1.0182437  1.1097597  1.2197654  1.1539668  1.0581696\n",
      " 1.1597503  1.0840323  1.1513782  1.0244596  1.091014   1.0822874\n",
      " 1.1712679  1.2209332  1.1128386  1.0741254  1.0868059  1.0689629\n",
      " 1.094564   1.1606944  1.0600553  1.0406435  1.062991   1.1072133\n",
      " 1.161964   1.0328957  1.1244663  1.004446   1.3151083  1.0764121\n",
      " 1.109694   1.1277274  1.0906733  1.1429464  1.0434757  1.1050308\n",
      " 1.1115907  1.0801213  1.056981   1.1035424  1.1339976  1.1456035\n",
      " 1.1466141  1.0851631  1.1252546  1.1252459  1.074109   1.122869\n",
      " 1.1168712  1.1636851  1.0965077  1.1153055  1.0218585  1.0474893\n",
      " 1.0187175  1.0817015  1.1172519  1.2452185  1.1059092  1.0628645 ]\n",
      "norm2-2/beta:0 [-0.06599762 -0.09049378 -0.13528243 -0.19146365 -0.21172722 -0.14082868\n",
      " -0.15201709 -0.2180529  -0.21434507 -0.07986821 -0.19629066 -0.15542375\n",
      " -0.16836685 -0.13373184 -0.13083841 -0.16300103 -0.09705201 -0.15952118\n",
      " -0.1828856  -0.13404277 -0.13189767 -0.11413965 -0.11155852 -0.12040406\n",
      " -0.16266288 -0.12218112 -0.14360775 -0.17838044 -0.20482148 -0.1701939\n",
      " -0.14465079 -0.13290493 -0.10187861 -0.1273764  -0.18414317 -0.09109873\n",
      " -0.10580533 -0.05051266 -0.05962848 -0.2530596  -0.21204574 -0.20173128\n",
      " -0.22461927 -0.226383   -0.23384441 -0.1220855  -0.07002928 -0.09001058\n",
      " -0.1723906  -0.17256355 -0.27051657 -0.16657609 -0.24734294 -0.10620254\n",
      " -0.08176242 -0.21926822 -0.29222503 -0.17909886 -0.27256355 -0.16547056\n",
      " -0.08680492 -0.1860365  -0.26862854 -0.21260281 -0.11905535 -0.14745207\n",
      " -0.12308399 -0.1721368  -0.16535491 -0.19500875 -0.20689583 -0.18507215\n",
      " -0.07416225 -0.1477875  -0.13951401 -0.1592413  -0.16259155 -0.2629682\n",
      " -0.15756121 -0.18743289 -0.18512781 -0.19459549 -0.11115821 -0.20786692\n",
      " -0.07355204 -0.21594551 -0.12483642 -0.12372154 -0.18829332 -0.05832395\n",
      " -0.13734913 -0.19162792 -0.14447632 -0.11505359 -0.05663469 -0.18662886\n",
      " -0.1514439  -0.21544573 -0.20319252 -0.21923655 -0.15628722 -0.21461077\n",
      " -0.14181897 -0.17558612 -0.23392655 -0.11005504 -0.18138868 -0.12503164\n",
      " -0.19096245 -0.14250593 -0.12953669 -0.17792147 -0.05104575 -0.0871901\n",
      " -0.15143755 -0.1639224  -0.22829463 -0.14486162 -0.22850367 -0.138513\n",
      " -0.0889223  -0.11570867 -0.13187897 -0.0991874  -0.1611675  -0.15745601\n",
      " -0.21818462 -0.12428807 -0.14433102 -0.22098643 -0.16176017 -0.09158225\n",
      " -0.0827012  -0.17196682 -0.13889231 -0.1716237  -0.16455276 -0.22503746\n",
      " -0.00548496 -0.16409339 -0.26154524 -0.20568736 -0.1507613  -0.29760584\n",
      " -0.09861863 -0.15615547 -0.18871687 -0.22896808 -0.19645663 -0.210971\n",
      " -0.10218169 -0.21673436 -0.12462834 -0.19598319 -0.16369197 -0.17249778\n",
      " -0.07704651 -0.18412516 -0.10147912 -0.18815969 -0.18778227 -0.19333932\n",
      " -0.12353424 -0.172018   -0.23500247 -0.25354916 -0.13568214 -0.14186238\n",
      " -0.2120516  -0.18984768 -0.23338915 -0.14851992 -0.18568033 -0.15013456\n",
      " -0.11525872 -0.19105358 -0.16654076 -0.15356699 -0.2031483  -0.14719813\n",
      " -0.22214268 -0.09511241 -0.12769583 -0.18378946 -0.24390973 -0.2111363\n",
      " -0.17582503 -0.15567133 -0.1644152  -0.13202858 -0.1538385  -0.15544648]\n",
      "Conv_4/weights:0 [[[[-4.34005022e-01 -4.50763740e-02  2.72642355e-02 ... -1.02931097e-01\n",
      "    -1.75943656e-03 -6.33548722e-02]\n",
      "   [-1.38510624e-02 -3.67289521e-02  5.94693087e-02 ... -2.06432976e-02\n",
      "     6.83249980e-02 -1.18840255e-01]\n",
      "   [ 1.35090843e-01 -1.26045927e-01 -4.40973938e-02 ...  2.43301392e-02\n",
      "    -1.66078836e-01 -1.21959873e-01]\n",
      "   ...\n",
      "   [ 2.03425344e-02 -2.18342900e-01  3.72822620e-02 ...  5.46133928e-02\n",
      "     1.22913979e-01 -1.24887735e-01]\n",
      "   [-8.76440331e-02  7.98084587e-02  1.23006433e-01 ... -2.44077474e-01\n",
      "     1.38828933e-01  1.57412440e-01]\n",
      "   [-1.91476405e-01  3.91550139e-02  3.14028263e-02 ... -1.53586464e-02\n",
      "    -1.64006859e-01  5.98839633e-02]]\n",
      "\n",
      "  [[-3.04437101e-01 -4.62160970e-04  2.25089788e-02 ... -1.20761104e-01\n",
      "    -7.04443529e-02  4.40297164e-02]\n",
      "   [ 8.89603943e-02 -8.38561207e-02 -8.69371593e-02 ... -1.27819210e-01\n",
      "     1.68433353e-01 -1.00657456e-01]\n",
      "   [ 1.31516710e-01 -1.50240377e-01 -7.88723826e-02 ...  3.87477465e-02\n",
      "     6.39778152e-02 -5.48591167e-02]\n",
      "   ...\n",
      "   [ 5.65176755e-02 -1.67734414e-01  4.11674827e-02 ... -2.87051890e-02\n",
      "     3.40067409e-02 -3.49977873e-02]\n",
      "   [ 1.68032497e-02 -1.04974695e-01  2.01441869e-01 ...  3.07788439e-02\n",
      "     1.47718742e-01  6.09159982e-03]\n",
      "   [-1.73061624e-01  9.01359916e-02 -1.27606206e-02 ...  1.47804553e-02\n",
      "    -6.52195811e-02  3.21970670e-03]]\n",
      "\n",
      "  [[-1.05936661e-01 -6.94513097e-02 -1.88338459e-01 ... -2.11779952e-01\n",
      "    -2.68883198e-01  1.66680515e-02]\n",
      "   [-1.60657242e-02 -2.90212967e-02 -7.97955319e-02 ... -9.48744267e-02\n",
      "     7.27825314e-02 -3.45015898e-02]\n",
      "   [ 2.49147296e-01 -1.67246968e-01 -1.16344988e-01 ...  3.67857255e-02\n",
      "     9.49968770e-02  3.48447971e-02]\n",
      "   ...\n",
      "   [ 6.41053449e-03  6.48301793e-03 -7.24202916e-02 ...  1.36089679e-02\n",
      "    -7.79367760e-02 -1.20116128e-02]\n",
      "   [ 1.71294242e-01 -1.08419038e-01  2.94838529e-02 ...  1.51673138e-01\n",
      "     1.26807436e-01  7.63732716e-02]\n",
      "   [ 1.16727501e-02  1.15677968e-01 -4.81691696e-02 ...  9.64698009e-03\n",
      "    -1.12889804e-01 -2.66640738e-04]]]\n",
      "\n",
      "\n",
      " [[[ 1.30397053e-02 -3.99806052e-02 -1.23247155e-03 ... -7.39184693e-02\n",
      "     8.08366388e-02 -5.58246337e-02]\n",
      "   [-1.88656077e-02 -5.83940744e-02  3.63850519e-02 ... -8.80462825e-02\n",
      "    -5.47329187e-02 -5.70894480e-02]\n",
      "   [ 2.35657562e-02 -1.08781494e-01 -1.14253961e-01 ...  4.13965918e-02\n",
      "    -1.42368749e-01 -2.42093466e-02]\n",
      "   ...\n",
      "   [-1.05563097e-01 -7.65689313e-02  1.76864177e-01 ...  1.70056984e-01\n",
      "    -6.66137114e-02 -9.41762701e-03]\n",
      "   [-6.54264838e-02 -4.64465432e-02 -9.91460122e-03 ... -8.07487518e-02\n",
      "     1.54609725e-01  2.94220392e-02]\n",
      "   [-6.14624955e-02 -1.06146075e-01  1.76979098e-02 ... -3.50209437e-02\n",
      "    -1.58204451e-01  5.50384261e-03]]\n",
      "\n",
      "  [[-9.00411606e-02 -1.62776578e-02 -1.27410159e-01 ... -1.67909618e-02\n",
      "     2.63194963e-02  1.73378773e-02]\n",
      "   [ 4.41451035e-02 -9.93362069e-02 -6.24096114e-03 ... -1.55591712e-04\n",
      "     1.64694443e-01 -3.92521843e-02]\n",
      "   [ 5.49949240e-04 -1.21437199e-01 -1.76753983e-01 ...  6.77517653e-02\n",
      "     2.73543932e-02 -8.81460831e-02]\n",
      "   ...\n",
      "   [-6.23618253e-02 -1.06277272e-01  2.22510114e-01 ...  4.26409841e-02\n",
      "    -8.48566145e-02 -2.74714269e-02]\n",
      "   [ 7.21223280e-02 -9.77386385e-02  1.29003059e-02 ...  4.19549011e-02\n",
      "     1.81768507e-01  2.09465297e-03]\n",
      "   [-1.07763998e-01  8.64191204e-02 -4.60306406e-02 ...  3.14236991e-02\n",
      "    -9.18518528e-02  3.01593021e-02]]\n",
      "\n",
      "  [[-9.76490155e-02 -3.57179530e-02 -2.23989621e-01 ... -7.95619637e-02\n",
      "    -7.35416040e-02 -5.10170050e-02]\n",
      "   [-1.48099557e-01 -1.04758330e-01  3.86605412e-02 ... -6.82472885e-02\n",
      "     1.61838949e-01 -6.48940876e-02]\n",
      "   [-1.58082053e-01 -8.02147686e-02 -1.05671607e-01 ...  2.65855324e-02\n",
      "     6.52864650e-02  2.55600158e-02]\n",
      "   ...\n",
      "   [-2.22457591e-02 -7.12310672e-02  1.49849117e-01 ...  2.41575569e-01\n",
      "    -1.12421691e-01 -4.58060019e-02]\n",
      "   [ 6.18534237e-02 -6.47066534e-02 -1.47366062e-01 ...  1.32694051e-01\n",
      "     2.18803793e-01 -1.90211069e-02]\n",
      "   [ 7.38508720e-03  1.71451196e-02  6.97083175e-02 ...  4.40861471e-02\n",
      "    -4.56906073e-02 -4.61573936e-02]]]\n",
      "\n",
      "\n",
      " [[[ 2.92667598e-02 -5.28897643e-02 -9.71892178e-02 ... -5.20041622e-02\n",
      "     7.55203888e-02 -1.33326218e-01]\n",
      "   [ 2.89373118e-02 -2.76326612e-02  5.12552261e-02 ... -2.55092364e-02\n",
      "    -4.41836454e-02  6.53837472e-02]\n",
      "   [ 1.76105663e-01  6.88683614e-03 -1.10467255e-01 ...  6.63194656e-02\n",
      "    -1.36415437e-01  1.09729856e-01]\n",
      "   ...\n",
      "   [-2.43752673e-01 -8.82384256e-02  4.65482706e-03 ...  7.25078508e-02\n",
      "    -1.12746134e-01 -1.06531858e-01]\n",
      "   [-1.33548349e-01 -8.99318233e-02 -9.07096639e-03 ... -1.41399652e-02\n",
      "     2.90284678e-02  3.66282836e-02]\n",
      "   [-4.27424647e-02 -1.35218188e-01 -1.50672346e-01 ... -2.03187880e-03\n",
      "    -1.26719013e-01  1.45090732e-03]]\n",
      "\n",
      "  [[ 5.96112013e-03 -2.55649146e-02 -1.70901135e-01 ... -3.19879316e-03\n",
      "     5.69500364e-02  1.81455910e-02]\n",
      "   [-3.17469984e-02 -7.12831318e-02  7.01809824e-02 ...  1.30240470e-01\n",
      "     2.92680025e-01 -2.35117842e-02]\n",
      "   [-8.47608224e-03 -3.41858529e-02 -1.33919388e-01 ...  1.91477733e-03\n",
      "    -1.52307451e-02  1.03464283e-01]\n",
      "   ...\n",
      "   [-2.05127433e-01 -4.33211178e-02 -8.67839623e-03 ... -3.07281781e-02\n",
      "    -3.39259386e-01 -2.27227792e-01]\n",
      "   [ 4.07009460e-02 -2.82941926e-02 -1.04639582e-01 ...  2.45160591e-02\n",
      "     2.97639016e-02  3.61322425e-02]\n",
      "   [ 5.06028011e-02  1.40321571e-02  3.53177590e-03 ... -4.96161245e-02\n",
      "     5.48402080e-03 -5.41231707e-02]]\n",
      "\n",
      "  [[-5.28853610e-02 -4.52826172e-02 -1.43735602e-01 ... -2.30966844e-02\n",
      "     4.45409212e-04 -4.84504253e-02]\n",
      "   [-6.81090206e-02 -1.16010122e-01  6.29309937e-02 ...  5.83204478e-02\n",
      "     1.29946128e-01 -2.92541552e-02]\n",
      "   [-1.59334868e-01  9.12507176e-02 -7.12039396e-02 ...  5.45390742e-03\n",
      "     3.00378114e-01  2.52020568e-01]\n",
      "   ...\n",
      "   [ 6.91660307e-03 -9.58807468e-02  2.21195836e-02 ... -5.37179131e-03\n",
      "    -2.25760669e-01 -3.16180110e-01]\n",
      "   [ 3.51625239e-03  4.76363786e-02 -2.15742096e-01 ...  7.14060143e-02\n",
      "     1.39088854e-01 -1.25634328e-01]\n",
      "   [ 8.15908983e-02 -4.81164269e-02  1.23031624e-01 ...  1.18399732e-01\n",
      "     4.25146669e-02 -1.08147010e-01]]]]\n",
      "Conv_4/biases:0 [-0.04445732  0.06122757  0.03830826  0.05407493  0.08007838  0.08872116\n",
      " -0.09175036 -0.09472499 -0.02224939  0.00748505 -0.12638487  0.01543561\n",
      "  0.12473028 -0.06729027  0.03275714 -0.04075572  0.03480474  0.08698881\n",
      "  0.09128463  0.15660943  0.07044547  0.04728822 -0.04199477  0.02418799\n",
      "  0.01850464 -0.16289905 -0.20543174 -0.03876084 -0.17660777  0.03718643\n",
      "  0.00197699 -0.01200797 -0.03487084  0.069397    0.09545401  0.01573806\n",
      "  0.00360135 -0.08534979 -0.0236137   0.04562084  0.03023017  0.05656631\n",
      "  0.00735633  0.02438918 -0.03658094 -0.06430907 -0.07222417  0.07163706\n",
      "  0.04057552  0.00479056 -0.07533347  0.12882338  0.01971524  0.01910179\n",
      "  0.0465438   0.07888605  0.1000404   0.10740985 -0.08541237  0.03076299\n",
      " -0.03671526  0.04350315  0.05838094  0.03804985 -0.09702821  0.03927202\n",
      "  0.03862695 -0.096876   -0.00138453 -0.09912577  0.02092743 -0.11092155\n",
      "  0.0120005   0.12681651  0.07026247  0.02861471  0.08983819 -0.08921371\n",
      "  0.03736413 -0.00599014  0.02178582 -0.02304805 -0.03768148 -0.0030525\n",
      " -0.10049525  0.0893718  -0.02203266  0.06067945 -0.02584125  0.0294373\n",
      "  0.0358067   0.02947937 -0.02724924 -0.07916668 -0.00982947  0.04769326\n",
      " -0.04073183 -0.00472719 -0.08536669  0.02348862  0.00810656  0.01298052\n",
      " -0.05122649  0.09817185 -0.03798826 -0.09322601  0.09205756 -0.08403851\n",
      " -0.04431981 -0.0141057   0.04371766  0.09619797  0.01422814  0.07811048\n",
      "  0.0928269   0.03014841  0.01352094 -0.00868598 -0.00620481  0.0191009\n",
      "  0.08153686  0.09420518  0.02377204 -0.05128764  0.0786668  -0.00091756\n",
      "  0.06569092  0.06324184  0.03786211 -0.0417273   0.00199862 -0.01845698\n",
      "  0.09256794 -0.11366846 -0.01436023  0.10547879 -0.02061416  0.0679587\n",
      " -0.03296467 -0.0427692   0.08347272  0.07750566  0.10702133 -0.3333002\n",
      "  0.12068934  0.04763662  0.02919428  0.00262347 -0.10527081 -0.03833546\n",
      "  0.09126756 -0.04650449 -0.06607424  0.04621197  0.00399069  0.05633289\n",
      " -0.05906621 -0.01286229 -0.06874429  0.01184119  0.02761889 -0.11970549\n",
      "  0.05861817  0.00494929 -0.00762796  0.06939902  0.00775095  0.14099596\n",
      " -0.01662493 -0.0135116   0.05087151 -0.14441358 -0.0473897   0.13678084\n",
      "  0.01406792 -0.08533456  0.00540508  0.04853746 -0.09123876  0.0011898\n",
      " -0.13159701 -0.03284623  0.02341827 -0.06438437  0.01209694  0.01800648\n",
      " -0.03145935  0.0117525  -0.03641566  0.01430955 -0.01343076  0.07767596]\n",
      "norm3/gamma:0 [1.2219013  1.0691302  1.0874376  0.9994059  1.1350964  1.1122895\n",
      " 1.2243267  1.1653025  1.2388151  1.118759   1.1824563  1.1869725\n",
      " 1.2093749  1.1723241  1.2364647  1.1884581  1.0193586  1.0565592\n",
      " 1.1275461  1.0643243  0.98468494 1.2117993  1.2561074  1.0437232\n",
      " 1.2935364  1.0459565  1.2255771  1.2726625  1.2483754  1.2434187\n",
      " 1.1027176  1.109479   1.1021116  1.0092646  1.0320203  0.97610337\n",
      " 0.99171966 1.3064587  1.2974534  1.0303793  1.0699875  1.2857999\n",
      " 1.0745548  1.2394589  1.1215818  1.1809793  1.1870481  1.1168331\n",
      " 1.0508593  1.1777371  1.2324901  1.0824953  1.1975483  1.1969702\n",
      " 1.0519556  1.1062504  1.256357   1.2350118  1.2215651  1.1482579\n",
      " 1.3954291  1.1950706  1.228098   1.2303748  1.1115315  1.322375\n",
      " 1.2575287  1.2849773  1.3268484  1.2923868  1.1773968  1.1319144\n",
      " 1.2610981  1.03921    1.0420935  1.1640563  1.0802684  1.2210897\n",
      " 1.2738607  1.1257608  1.0817218  0.9831479  1.1474446  1.0715151\n",
      " 1.1829941  1.0205041  1.1005613  1.1276865  1.1826297  1.1925875\n",
      " 1.1588769  1.2138714  0.99756765 1.3136687  1.2130021  1.0861273\n",
      " 1.1951149  1.2495545  1.2153327  1.1486353  1.2707142  1.1616462\n",
      " 1.2236644  1.0967703  1.2600263  1.2219123  0.99734825 1.1853299\n",
      " 1.1123811  1.2940845  1.1186162  1.0818406  0.99119645 1.0835605\n",
      " 1.1260927  0.98591214 1.1925659  1.0274879  1.3421186  1.034671\n",
      " 1.1398616  1.3541209  1.2038232  1.0190499  1.134697   0.96661866\n",
      " 1.175271   1.0119531  1.1692114  1.3668915  1.184075   0.9983524\n",
      " 1.0306747  1.2746533  0.9200709  1.0943623  1.2351462  1.0966725\n",
      " 1.1102494  1.1920838  1.2091092  0.97646487 1.262635   0.9174355\n",
      " 1.1301336  1.0537864  0.9722898  1.2225331  1.2285571  0.96505845\n",
      " 1.1408302  1.1476774  1.2508354  1.2156017  1.0357285  1.135524\n",
      " 1.2778693  1.0287507  1.3140452  1.2038269  0.9672224  1.2217895\n",
      " 1.1848118  1.3488992  1.129805   1.0618263  1.1867617  1.1635499\n",
      " 1.1902694  1.1263082  1.0674788  1.2316847  1.1595067  1.0360949\n",
      " 1.1128588  1.2695951  1.1745142  1.2600825  1.3149486  1.0010991\n",
      " 1.2700706  1.0508128  1.1047983  1.1759796  1.1526482  1.1320112\n",
      " 1.2337939  1.1543493  1.149753   1.1178186  1.3170873  1.1768212 ]\n",
      "norm3/beta:0 [-0.15447807 -0.05942645 -0.20091632  0.00181667 -0.01949061 -0.02012475\n",
      " -0.26880962 -0.1203044   0.06648498 -0.22009557 -0.0197696  -0.05778982\n",
      "  0.03845049 -0.05980843 -0.06043723  0.03628108 -0.10344659 -0.09940471\n",
      " -0.02978075  0.01670095 -0.00790451 -0.08238681 -0.01212832 -0.0744468\n",
      " -0.06359571 -0.16751915 -0.24866162  0.07438067 -0.13809603 -0.05839734\n",
      " -0.15027313 -0.19976078 -0.28177062 -0.05714487  0.01098984 -0.01545725\n",
      " -0.1253391  -0.25750712 -0.22708131 -0.09031241 -0.20281097 -0.00288811\n",
      " -0.19411951  0.06100535 -0.20847546  0.01890969  0.02779048 -0.03130693\n",
      " -0.0364594  -0.09780443 -0.15135427  0.02109802 -0.12617543  0.02728365\n",
      " -0.01867991 -0.10713684  0.09181777  0.01111512 -0.21139492 -0.11430073\n",
      "  0.04609197 -0.00832733 -0.00985848  0.02989576 -0.13913642 -0.04995767\n",
      " -0.0100776  -0.16862954 -0.14821574 -0.21972385 -0.0783212  -0.3643176\n",
      "  0.05232951  0.02297248 -0.12574248 -0.10889652 -0.10365304 -0.1943625\n",
      " -0.10113977 -0.15681508 -0.17299026 -0.05977965 -0.09316793 -0.17122512\n",
      " -0.2050558  -0.18412802 -0.106242   -0.0676778  -0.16197208 -0.20611513\n",
      " -0.07717716 -0.003466   -0.14337723 -0.0414582  -0.19043905 -0.17463088\n",
      " -0.14268097  0.07992296 -0.04984087 -0.05012177 -0.10189505 -0.00283506\n",
      " -0.06691352 -0.06104431 -0.00543062 -0.3403831  -0.01939144 -0.14089268\n",
      " -0.10840527 -0.10821214 -0.07052621 -0.00102467 -0.12816244 -0.09078727\n",
      " -0.00871028 -0.10984289  0.07482371 -0.26180786 -0.07884111 -0.036459\n",
      " -0.05700526  0.08827689  0.01843491 -0.19346043 -0.00966128 -0.06348307\n",
      " -0.1807689  -0.03911835 -0.06138057  0.01130468 -0.09865703 -0.18672244\n",
      " -0.02338547 -0.06291535 -0.10478421  0.04616421 -0.00396409 -0.10352323\n",
      " -0.10205448 -0.09056323 -0.0762157  -0.02401914 -0.00887242 -0.3801341\n",
      "  0.03266069 -0.11242015 -0.0479334  -0.21850105 -0.1028975  -0.10908694\n",
      " -0.02487478 -0.01798376  0.00069968 -0.07035613 -0.13755839 -0.06655231\n",
      " -0.21086632 -0.14330865 -0.1262354  -0.06474059 -0.13624835 -0.15918413\n",
      " -0.06916909 -0.05935894 -0.17124079 -0.03139639 -0.05898361 -0.04097836\n",
      " -0.08490428 -0.20146625 -0.1346513  -0.24891132 -0.0062339   0.05307766\n",
      " -0.2545336  -0.28334448 -0.12330305 -0.02270233 -0.10970012 -0.08979951\n",
      " -0.13012199 -0.24703106 -0.19550182 -0.24309847 -0.20580216 -0.21377501\n",
      " -0.10227619 -0.16885424 -0.16303156 -0.12382946 -0.09216104 -0.03543497]\n",
      "Conv_5/weights:0 [[[[-0.08943706 -0.28527957 -0.25081226 ... -0.14640844 -0.08013001\n",
      "    -0.18547855]\n",
      "   [-0.07568521  0.08923174 -0.12627573 ... -0.11355055  0.1769963\n",
      "    -0.14189413]\n",
      "   [ 0.00956847 -0.09860136 -0.21336477 ... -0.01775952 -0.07221858\n",
      "    -0.18852337]\n",
      "   ...\n",
      "   [ 0.02565751 -0.03830462  0.14951189 ...  0.00559616  0.02168581\n",
      "    -0.04907094]\n",
      "   [-0.0533249  -0.3601475  -0.1908009  ...  0.07335635  0.05074312\n",
      "    -0.05710359]\n",
      "   [-0.1340925   0.03215621 -0.0875568  ... -0.06017738  0.19935094\n",
      "    -0.10066704]]]]\n",
      "Conv_5/biases:0 [ 4.95319851e-02  1.27407134e-01  1.33155519e-02  7.35302642e-02\n",
      "  3.91502529e-02  1.33573741e-01  9.90680829e-02  1.26170546e-01\n",
      "  7.32716247e-02  3.04046478e-02  5.54316714e-02  7.80037418e-02\n",
      "  6.74220920e-02  1.25686690e-01 -1.04868278e-01  1.51593417e-01\n",
      "  2.13122237e-02  7.24353641e-02  1.52512789e-01  9.18067992e-02\n",
      " -8.33250582e-02  1.99086480e-02  4.55922782e-02  1.31450012e-01\n",
      "  9.88257900e-02  3.36047970e-02 -2.06910949e-02  1.14345476e-01\n",
      "  5.85105196e-02 -3.08919960e-04  5.11154644e-02  8.39323103e-02\n",
      "  1.32844612e-01  1.79085713e-02  1.08459689e-01  1.54309407e-01\n",
      "  8.91076252e-02  2.39019226e-02 -3.44335884e-02 -7.13535771e-02\n",
      " -6.22681230e-02 -1.55297428e-01 -5.56822459e-04  7.54544735e-02\n",
      " -1.02644991e-02  1.58996508e-01  7.50679001e-02  1.93076022e-02\n",
      "  9.16405320e-02  6.32682592e-02  1.39238432e-01  5.26024885e-02\n",
      "  6.00593612e-02 -3.05456836e-02  1.10442325e-01  1.08200058e-01\n",
      "  3.12648863e-02  5.45864142e-02  7.04436973e-02  1.18711032e-01\n",
      " -6.73974603e-02  8.12861100e-02  4.85056378e-02  1.17718801e-02\n",
      "  8.88701007e-02 -6.29771501e-02  3.44867967e-02 -2.33929940e-02\n",
      "  3.02642751e-02  3.00157890e-02  1.10151730e-01  1.34301990e-01\n",
      "  7.79159963e-02  1.17893025e-01  2.24388279e-02  2.85717542e-03\n",
      "  5.29512316e-02  1.15711354e-01  4.07663137e-02  1.37472808e-01\n",
      "  4.28754985e-02  1.65949538e-01  8.79078060e-02  6.04957119e-02\n",
      "  3.97301316e-02  5.17796353e-02  1.03060521e-01  5.68050221e-02\n",
      "  2.97088195e-02  8.25282000e-03  4.46228832e-02  1.83682963e-01\n",
      " -1.19688481e-01  3.43080238e-03  3.74258123e-03  8.29630718e-02\n",
      " -9.49810222e-02  8.08745325e-02  9.04334560e-02  3.37003767e-02\n",
      "  7.84617662e-02  4.37796526e-02  6.32709488e-02  1.39383804e-02\n",
      "  1.09276026e-01 -9.08657070e-03  5.93204983e-02  1.01549730e-01\n",
      "  9.88715142e-02  1.17422223e-01 -4.19050083e-03 -1.61893666e-01\n",
      "  9.16151702e-02  1.30522791e-02  2.17679963e-02  6.86446875e-02\n",
      "  5.19893430e-02 -1.65769632e-03  4.15626913e-02  1.21553190e-01\n",
      "  6.37083575e-02  3.33318375e-02  4.28484119e-02  6.02004975e-02\n",
      "  7.33579881e-03  2.31410060e-02  3.74237522e-02  9.00817364e-02\n",
      "  1.97849870e-02  1.13852032e-01  3.52178961e-02  4.86902706e-02\n",
      "  9.03654248e-02 -2.83430200e-02  1.60625935e-01 -4.34868932e-02\n",
      " -2.71742865e-02  6.20260611e-02  1.30113512e-01  8.82305279e-02\n",
      "  1.63625509e-01  1.22850366e-01 -1.26671614e-04  1.33797880e-02\n",
      "  9.84014049e-02  1.34270459e-01  4.94090170e-02  9.11320671e-02\n",
      "  9.74726230e-02  1.94368362e-02 -8.63769948e-02  4.38490063e-02\n",
      " -5.36550321e-02  7.51100555e-02  1.21142089e-01  4.63674776e-02\n",
      "  9.72461626e-02  1.55680463e-01  6.15082495e-02  7.48978928e-02\n",
      "  8.50421861e-02  7.57402703e-02  7.83357769e-02  4.83576953e-02\n",
      "  8.06347430e-02  4.29647118e-02  8.20534453e-02 -4.51623946e-02\n",
      "  8.17958266e-02  5.31128943e-02  1.42641917e-01  8.62131491e-02\n",
      " -2.41046418e-02  1.89945512e-02  1.14719768e-03  1.06252335e-01\n",
      "  4.40927632e-02  1.10616632e-01  1.59186441e-02 -6.04702253e-03\n",
      "  3.22487280e-02  5.63890710e-02  9.80872512e-02  7.19519258e-02\n",
      "  5.53416088e-02  1.28949150e-01  1.30154729e-01  1.00753829e-01\n",
      "  8.57824311e-02  7.58640692e-02  1.11286752e-01 -1.32028079e-02]\n",
      "norm4/gamma:0 [1.1532322  1.0860813  1.1183206  1.1387306  1.2170123  1.2311995\n",
      " 1.1724739  1.2647609  1.127174   1.1848899  1.1187918  1.0568336\n",
      " 1.1636204  1.1432604  1.0459598  1.209082   1.0123531  1.0926586\n",
      " 1.1268854  1.2226914  0.95102733 1.1016736  1.0806575  1.1677256\n",
      " 1.134124   1.1085469  1.0203001  1.0432669  1.0736085  0.99064237\n",
      " 1.1953087  1.1869314  1.0816638  1.0265471  1.1958892  1.1846615\n",
      " 1.178464   0.99671406 1.0581276  0.9126201  1.0094224  1.0575722\n",
      " 1.0277902  1.1575963  1.0332384  1.123128   1.1964518  1.035312\n",
      " 1.2658156  1.134553   1.2511057  1.2624884  1.1715431  1.1031662\n",
      " 1.1934633  1.3027357  1.2167673  1.1462868  1.0573926  1.1046172\n",
      " 1.1106513  1.1148587  1.0418135  1.0110621  1.2226652  0.9249334\n",
      " 1.2127725  1.0564169  1.0743626  1.0780293  1.0333947  1.1659645\n",
      " 1.119978   1.1949239  1.0785675  1.0334946  1.0417726  1.2364966\n",
      " 1.0409975  1.1956909  1.1562529  1.2372279  1.1408061  1.0348089\n",
      " 1.085232   1.157445   1.0449864  1.2847005  1.1895443  1.2055167\n",
      " 1.0428813  1.0946484  0.92191297 1.0639656  1.0307432  1.381508\n",
      " 1.0813504  1.1820807  1.1950306  1.1345643  1.0308526  0.994865\n",
      " 1.1123552  1.0046694  1.1615317  1.0667769  1.1691909  1.2882048\n",
      " 1.1779655  1.048739   1.0747659  0.8798664  1.2412962  1.0023735\n",
      " 1.097964   1.1517638  1.0441551  1.0271304  1.1623551  1.0919\n",
      " 1.3481672  1.028553   1.0111718  1.1076162  1.0048108  1.0681324\n",
      " 1.0628482  1.1595144  1.1951028  1.1100804  1.0653393  1.0602611\n",
      " 1.2171708  1.0132532  1.1302195  1.1177555  1.1685361  1.0840483\n",
      " 1.192527   1.0362664  1.191135   1.1804225  1.2491546  0.9935362\n",
      " 1.1788701  1.2051888  1.0472445  1.0307581  1.2382421  1.0790471\n",
      " 0.95946664 1.1690543  1.0277969  1.3023777  1.1949332  1.0798028\n",
      " 1.0311899  1.2807461  1.139332   1.2284344  1.0528835  1.043494\n",
      " 1.0223199  1.0223769  1.2374196  1.1054884  1.1190975  1.0441543\n",
      " 1.1886909  1.0390292  1.2186594  1.1604524  1.0544437  1.0577719\n",
      " 0.9857171  1.0807729  1.1846142  1.1310158  0.99644256 1.0385237\n",
      " 0.9806382  1.2388544  1.1803126  1.2353001  1.2257905  1.0870023\n",
      " 1.1558192  1.2920041  1.0469513  1.1187718  1.215039   1.0786166 ]\n",
      "norm4/beta:0 [-0.02088905 -0.00215558 -0.06409396 -0.12697002 -0.04206818  0.04646422\n",
      "  0.03159621  0.04967197  0.03205916 -0.0407595  -0.05715219  0.02112331\n",
      "  0.028641    0.04890547 -0.06097364  0.03732765 -0.04607294 -0.07899114\n",
      "  0.06727535 -0.00384925 -0.23307718 -0.1431006   0.00023309  0.04030367\n",
      " -0.00403585 -0.09007461 -0.00504077  0.05243436  0.01675888 -0.05076503\n",
      " -0.05764001 -0.04493079 -0.03643105 -0.03493256 -0.05102762  0.04270417\n",
      " -0.00305485 -0.00899877 -0.13062331 -0.0364849  -0.14838584 -0.12259902\n",
      " -0.13321884 -0.02145271 -0.11122179 -0.02754495 -0.03693806 -0.07006535\n",
      " -0.08167493 -0.18671097 -0.04567345 -0.04856622 -0.038389   -0.05545671\n",
      " -0.06120979  0.02951704 -0.0627712  -0.05265231 -0.03511875  0.0075079\n",
      " -0.12840556 -0.01547082 -0.02621701  0.02247539 -0.03577618 -0.02953433\n",
      " -0.16529356 -0.1829424  -0.08357232 -0.01350449  0.05843035 -0.0152005\n",
      " -0.10271487 -0.00476798 -0.08852656 -0.05989786  0.00711848  0.02076315\n",
      " -0.00215469  0.00971414 -0.10601101  0.10292145  0.04899516  0.00625115\n",
      " -0.04460486 -0.0488181   0.05445451 -0.07318887 -0.09728636 -0.1836488\n",
      " -0.01743215 -0.03330923 -0.08992188 -0.08412565 -0.05748141  0.0064584\n",
      " -0.09804212 -0.02588856 -0.05069675  0.06014918  0.03644071 -0.00113975\n",
      " -0.0365411  -0.04011718 -0.06337153 -0.15088557 -0.08810601 -0.02690519\n",
      " -0.04827726  0.06764258 -0.10844899 -0.01850598 -0.03344407  0.02054134\n",
      " -0.06949544  0.00387469 -0.02087519 -0.12017687 -0.00743019 -0.00757717\n",
      "  0.01348382 -0.05296192 -0.00275582 -0.11442345 -0.06094455 -0.04904505\n",
      " -0.10439824 -0.21568419 -0.10300244  0.07420155 -0.01866334 -0.007891\n",
      " -0.02778847 -0.04379259 -0.00403289 -0.0899862  -0.17904978  0.04563751\n",
      " -0.01473745 -0.05026197  0.03540312 -0.04710899 -0.08547182 -0.02473506\n",
      "  0.0206409   0.07193778  0.03215933  0.03866437  0.01583611 -0.14935552\n",
      " -0.08133703 -0.12022055  0.04188976  0.08014023  0.02594013 -0.02266054\n",
      "  0.05599716  0.0308831  -0.00468837  0.0496114  -0.08950348  0.02247286\n",
      "  0.04584324 -0.01041813 -0.02242448 -0.12626342  0.02067819 -0.04709497\n",
      " -0.02668683  0.01537308  0.06635068 -0.00174728 -0.07642657 -0.04021841\n",
      " -0.07454707 -0.08560434 -0.07618939  0.02404899 -0.05701766 -0.04455112\n",
      " -0.0156814  -0.08188928  0.01135429 -0.02220566 -0.05198835 -0.02546664\n",
      " -0.05604866 -0.09324452  0.01503984 -0.01158394 -0.0206115   0.03948943]\n",
      "Conv_6/weights:0 [[[[ 0.07266353  0.30015886  0.08294964 ... -0.00879791  0.0165282\n",
      "     0.08283538]\n",
      "   [ 0.07696326 -0.09070993  0.01495856 ... -0.3775007   0.15316081\n",
      "    -0.1272742 ]\n",
      "   [ 0.07655726 -0.02334003  0.16477196 ...  0.07696579  0.0456202\n",
      "     0.12313589]\n",
      "   ...\n",
      "   [ 0.13194372  0.27767763  0.02858013 ...  0.07907198  0.13496324\n",
      "     0.10089328]\n",
      "   [ 0.08891208 -0.20865156 -0.16129862 ...  0.32005078  0.03151722\n",
      "    -0.14975712]\n",
      "   [ 0.2221813  -0.191413    0.17446817 ...  0.00235914 -0.03067503\n",
      "     0.07964683]]]]\n",
      "Conv_6/biases:0 [0.06890038 0.003875   0.11695069 0.07842906 0.11631407 0.04821043\n",
      " 0.08237442 0.01458229 0.10177241 0.03828672]\n",
      "norm5/gamma:0 [1.1004725 1.1027718 1.1468003 1.092608  1.1163762 1.1227107 1.1253356\n",
      " 1.1255343 1.1349711 1.088159 ]\n",
      "norm5/beta:0 [ 0.13639987 -0.15338768 -0.06030349 -0.0838786   0.01195878 -0.0430423\n",
      " -0.09933794 -0.15247932  0.18317975  0.23382354]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Resets the session and restores the saved model\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, model_path_pruned.format(0.9))\n",
    "    tvars = tf.trainable_variables()\n",
    "    tvars_vals = sess.run(tvars)\n",
    "\n",
    "    for var, val in zip(tvars, tvars_vals):\n",
    "        print(var.name, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tvars = tf.trainable_variables()\n",
    "    for var in tvars:\n",
    "        if var.name.endswith('weights:0'):\n",
    "            val = sess.run(var)\n",
    "            print(var.name, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 512 # Entire training set\n",
    "model_path_unpruned = \"Model_Saves/Unpruned.ckpt\"\n",
    "model_path_pruned = \"Model_Saves/Pruned.ckpt\"\n",
    "NUM_CLASS = 10\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Import dataset\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "batches = int(len(mnist.train.images) / batch_size)\n",
    "\n",
    "# Define Placeholders\n",
    "# image = tf.placeholder(tf.float32, [None, 28,28])\n",
    "# label = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "image = tf.placeholder(name='images', dtype=tf.float32, shape=[None, 28, 28, 1])\n",
    "label = tf.placeholder(name='fine_labels', dtype=tf.int32, shape=[None,10])\n",
    "\n",
    "# # Define the model\n",
    "# layer1 = layers.masked_fully_connected(image, 300)\n",
    "# layer2 = layers.masked_fully_connected(layer1, 300)\n",
    "# logits = layers.masked_fully_connected(layer2, 10)\n",
    "\n",
    "# Define the model\n",
    "# layer1 = layers.masked_conv2d(image, 300, kernel_size=2)\n",
    "# layer2 = layers.masked_conv2d(layer1, 300, kernel_size=2)\n",
    "# logits = layers.masked_fully_connected(layer2, 10)\n",
    "\n",
    "_=image\n",
    "_ = layers.masked_conv2d(_, 96, (3, 3), 1, 'SAME')\n",
    "_ = tf.layers.batch_normalization(_, name='norm1-1')\n",
    "_ = layers.masked_conv2d(_, 96, (3, 3), 1, 'SAME')\n",
    "_ = tf.layers.batch_normalization(_, name='norm1-2')\n",
    "_ = tf.layers.max_pooling2d(_, (3, 3), 2, 'SAME',name='pool1')\n",
    "_ = layers.masked_conv2d(_, 192, (3, 3), 1, 'SAME')\n",
    "_ = tf.layers.batch_normalization(_, name='norm2-1')\n",
    "_ = layers.masked_conv2d(_, 192, (3, 3), 1, 'SAME')\n",
    "_ = tf.layers.batch_normalization(_, name='norm2-2')\n",
    "_ = tf.layers.max_pooling2d(_, (3, 3), 2, 'SAME', name='pool2')\n",
    "_ = layers.masked_conv2d(_, 192, (3, 3), 1, 'VALID')\n",
    "_ = tf.layers.batch_normalization(_, name='norm3')\n",
    "_ = layers.masked_conv2d(_, 192, (1, 1), 1)\n",
    "_ = tf.layers.batch_normalization(_, name='norm4')\n",
    "_ = layers.masked_conv2d(_, 10, (1, 1), 1)\n",
    "_ = tf.layers.batch_normalization(_, name='norm5')\n",
    "_ = tf.layers.average_pooling2d(_, (5,5), 1, name='avg_pool')\n",
    "y = _\n",
    "logits = tf.reshape(y,[tf.shape(y)[0],10])\n",
    "\n",
    "\n",
    "# Create global step variable (needed for pruning)\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "reset_global_step_op = tf.assign(global_step, 0)\n",
    "\n",
    "# Loss function\n",
    "#loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=label))\n",
    "#loss = tf.losses.softmax_cross_entropy(tf.one_hot(label, NUM_CLASS), logits)\n",
    "loss = tf.losses.softmax_cross_entropy(label, logits)\n",
    "\n",
    "# Training op, the global step is critical here, make sure it matches the one used in pruning later\n",
    "# running this operation increments the global_step\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss, global_step=global_step)\n",
    "\n",
    "# Accuracy ops\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get, Print, and Edit Pruning Hyperparameters\n",
    "pruning_hparams = pruning.get_pruning_hparams()\n",
    "print(\"Pruning Hyperparameters:\", pruning_hparams)\n",
    "\n",
    "# Change hyperparameters to meet our needs\n",
    "pruning_hparams.begin_pruning_step = 0\n",
    "pruning_hparams.end_pruning_step = 250\n",
    "pruning_hparams.pruning_frequency = 1\n",
    "pruning_hparams.sparsity_function_end_step = 250\n",
    "pruning_hparams.target_sparsity = .5\n",
    "\n",
    "# Create a pruning object using the pruning specification, sparsity seems to have priority over the hparam\n",
    "p = pruning.Pruning(pruning_hparams, global_step=global_step) #sparsity=.5)\n",
    "prune_op = p.conditional_mask_update_op()\n",
    "\n",
    "# Create a saver for writing training checkpoints.\n",
    "saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    # Uncomment the following if you don't have a trained model yet\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Train the model before pruning (optional)\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            #print(batch_xs.shape)\n",
    "            batch_xs = batch_xs.reshape(-1,28,28,1)\n",
    "            sess.run(train_op, feed_dict={image: batch_xs, label: batch_ys})\n",
    "\n",
    "        # Calculate Test Accuracy every 10 epochs\n",
    "        if epoch % 1 == 0:\n",
    "            acc_print = 0\n",
    "            for batch in range(batches):\n",
    "                batch_xt, batch_yt = mnist.test.next_batch(batch_size)\n",
    "                #print(batch_xt.shape)\n",
    "                batch_xt = batch_xt.reshape(-1,28,28,1)\n",
    "                acc_print += sess.run(accuracy, feed_dict={image: batch_xt, label: batch_yt})\n",
    "            print(\"Un-pruned model step %d test accuracy %g\" % (epoch, acc_print/batches))\n",
    "        print(epoch)\n",
    "    \n",
    "    # Saves the model before pruning\n",
    "    saver.save(sess, model_path_unpruned)\n",
    "    \n",
    "    acc_print = sess.run(accuracy, feed_dict={image: mnist.test.images.reshape(-1,28,28,1), label: mnist.test.labels})\n",
    "    print(\"Pre-Pruning accuracy:\", acc_print)\n",
    "    print(\"Sparsity of layers (should be 0)\", sess.run(tf.contrib.model_pruning.get_weight_sparsity()))\n",
    "\n",
    "    \n",
    "\n",
    "    # Resets the session and restores the saved model\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    saver.restore(sess, model_path_unpruned)\n",
    "\n",
    "    # Reset the global step counter and begin pruning\n",
    "    sess.run(reset_global_step_op)\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            batch_xs = batch_xs.reshape(-1,28,28,1)\n",
    "            # Prune and retrain\n",
    "            sess.run(prune_op)\n",
    "            sess.run(train_op, feed_dict={image: batch_xs, label: batch_ys})\n",
    "\n",
    "        # Calculate Test Accuracy every 10 epochs\n",
    "        if epoch % 1 == 0:\n",
    "            acc_print = 0\n",
    "            for batch in range(batches):\n",
    "                batch_xt, batch_yt = mnist.test.next_batch(batch_size)\n",
    "                #print(batch_xt.shape)\n",
    "                batch_xt = batch_xt.reshape(-1,28,28,1)\n",
    "                acc_print += sess.run(accuracy, feed_dict={image: batch_xt, label: batch_yt})\n",
    "            \n",
    "            print(\"Pruned model step %d test accuracy %g\" % (epoch, acc_print/batches))\n",
    "            print(\"Weight sparsities:\", sess.run(tf.contrib.model_pruning.get_weight_sparsity()))\n",
    "        print(epoch) \n",
    "        \n",
    "           # acc_print = sess.run(accuracy, feed_dict={image: mnist.test.images, label: mnist.test.labels})\n",
    "\n",
    "    # Saves the model after pruning\n",
    "    saver.save(sess, model_path_pruned)\n",
    "\n",
    "    # Print final accuracy\n",
    "    #acc_print = sess.run(accuracy, feed_dict={image: mnist.test.images, label: mnist.test.labels})\n",
    "    print(\"Final accuracy:\", acc_print)\n",
    "    print(\"Final sparsity by layer (should be 0)\", sess.run(tf.contrib.model_pruning.get_weight_sparsity()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
